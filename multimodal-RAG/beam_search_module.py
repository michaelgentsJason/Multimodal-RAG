#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Beam Searchå³æ’å³ç”¨æ¨¡å—
å¯ä»¥è½»æ¾é›†æˆåˆ°ç°æœ‰çš„å¤šæ„å›¾æ£€ç´¢ç³»ç»Ÿä¸­

ä½¿ç”¨æ–¹æ³•:
from beam_search_module import BeamSearchWrapper

# åŒ…è£…ä½ çš„ç°æœ‰æ£€ç´¢å™¨
wrapped_retriever = BeamSearchWrapper(
    base_retriever=your_retriever,
    matcher=your_matcher,
    reranker=your_reranker,
    enable_beam_search=True,  # å¼€å…³æ§åˆ¶
    beam_width=3
)

# åƒä½¿ç”¨åŸæ£€ç´¢å™¨ä¸€æ ·ä½¿ç”¨
results = wrapped_retriever.search_retrieval(data, retriever=matcher)
"""

import os
import sys
import json
import time
import heapq
import numpy as np
from typing import List, Dict, Tuple, Optional, Any, Union
from dataclasses import dataclass, field
from collections import defaultdict
import logging

logger = logging.getLogger(__name__)


@dataclass
class BeamSearchConfig:
    """Beam Searché…ç½®ç±»"""
    # åŸºç¡€é…ç½®
    enable: bool = True  # æ˜¯å¦å¯ç”¨Beam Search
    beam_width: int = 3  # beamå®½åº¦

    # æ„å›¾æ‹†è§£é…ç½®
    max_decomposition_methods: int = 4  # æœ€å¤§æ‹†è§£æ–¹æ³•æ•°
    enable_detailed_decomposition: bool = True  # æ˜¯å¦å¯ç”¨ç»†ç²’åº¦æ‹†è§£
    enable_coarse_decomposition: bool = True  # æ˜¯å¦å¯ç”¨ç²—ç²’åº¦æ‹†è§£
    enable_hierarchical_decomposition: bool = True  # æ˜¯å¦å¯ç”¨å±‚æ¬¡åŒ–æ‹†è§£

    # è¯„åˆ†æƒé‡
    diversity_weight: float = 0.25  # å¤šæ ·æ€§æƒé‡
    coverage_weight: float = 0.35  # è¦†ç›–åº¦æƒé‡
    relevance_weight: float = 0.30  # ç›¸å…³æ€§æƒé‡
    intent_quality_weight: float = 0.10  # æ„å›¾è´¨é‡æƒé‡

    # é˜ˆå€¼é…ç½®
    min_path_score: float = 0.05  # æœ€å°è·¯å¾„åˆ†æ•°é˜ˆå€¼
    similarity_threshold: float = 0.7  # è·¯å¾„ç›¸ä¼¼åº¦é˜ˆå€¼

    # ç»“æœé…ç½®
    max_final_results: int = 15  # æœ€å¤§æœ€ç»ˆç»“æœæ•°
    enable_result_reranking: bool = True  # æ˜¯å¦å¯ç”¨ç»“æœé‡æ’åº

    # è°ƒè¯•é…ç½®
    debug_mode: bool = False  # è°ƒè¯•æ¨¡å¼
    log_paths: bool = False  # æ˜¯å¦è®°å½•è·¯å¾„ä¿¡æ¯


@dataclass
class SearchPath:
    """æœç´¢è·¯å¾„ç±»"""
    path_id: str
    intent_decomposition: List[str]
    retrieval_results: List[Dict[str, Any]]
    scores: Dict[str, float] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)

    @property
    def total_score(self) -> float:
        return self.scores.get('total', 0.0)

    def __lt__(self, other):
        return self.total_score > other.total_score  # ç”¨äºä¼˜å…ˆé˜Ÿåˆ—ï¼Œåˆ†æ•°é«˜çš„ä¼˜å…ˆ


class IntentDecomposer:
    """æ„å›¾æ‹†è§£å™¨"""

    def __init__(self, base_retriever, config: BeamSearchConfig):
        self.base_retriever = base_retriever
        self.config = config

    def decompose_with_beam_search(self, query: str) -> List[Tuple[List[str], float, str]]:
        """ä½¿ç”¨Beam Searchè¿›è¡Œæ„å›¾æ‹†è§£"""
        decomposition_candidates = []

        # æ–¹æ³•1: æ ‡å‡†æ‹†è§£ï¼ˆå§‹ç»ˆå¯ç”¨ï¼‰
        try:
            standard_intents = self.base_retriever._split_query_intent(query)
            score1 = self._score_decomposition(query, standard_intents)
            decomposition_candidates.append((standard_intents, score1, "standard"))

            if self.config.debug_mode:
                logger.info(f"æ ‡å‡†æ‹†è§£: {len(standard_intents)} ä¸ªæ„å›¾, åˆ†æ•°: {score1:.3f}")
        except Exception as e:
            logger.warning(f"æ ‡å‡†æ‹†è§£å¤±è´¥: {e}")
            # å¤‡ç”¨æ–¹æ¡ˆ
            decomposition_candidates.append(([query], 0.5, "fallback"))

        # æ–¹æ³•2: ç»†ç²’åº¦æ‹†è§£
        if self.config.enable_detailed_decomposition:
            try:
                detailed_intents = self._generate_detailed_decomposition(query)
                score2 = self._score_decomposition(query, detailed_intents)
                decomposition_candidates.append((detailed_intents, score2, "detailed"))

                if self.config.debug_mode:
                    logger.info(f"ç»†ç²’åº¦æ‹†è§£: {len(detailed_intents)} ä¸ªæ„å›¾, åˆ†æ•°: {score2:.3f}")
            except Exception as e:
                logger.warning(f"ç»†ç²’åº¦æ‹†è§£å¤±è´¥: {e}")

        # æ–¹æ³•3: ç²—ç²’åº¦æ‹†è§£
        if self.config.enable_coarse_decomposition:
            try:
                if len(decomposition_candidates) > 0:
                    base_intents = decomposition_candidates[0][0]  # ä½¿ç”¨æ ‡å‡†æ‹†è§£ä½œä¸ºåŸºç¡€
                    coarse_intents = self._generate_coarse_decomposition(base_intents)
                    score3 = self._score_decomposition(query, coarse_intents)
                    decomposition_candidates.append((coarse_intents, score3, "coarse"))

                    if self.config.debug_mode:
                        logger.info(f"ç²—ç²’åº¦æ‹†è§£: {len(coarse_intents)} ä¸ªæ„å›¾, åˆ†æ•°: {score3:.3f}")
            except Exception as e:
                logger.warning(f"ç²—ç²’åº¦æ‹†è§£å¤±è´¥: {e}")

        # æ–¹æ³•4: å±‚æ¬¡åŒ–æ‹†è§£
        if self.config.enable_hierarchical_decomposition:
            try:
                hierarchical_intents = self._generate_hierarchical_decomposition(query)
                score4 = self._score_decomposition(query, hierarchical_intents)
                decomposition_candidates.append((hierarchical_intents, score4, "hierarchical"))

                if self.config.debug_mode:
                    logger.info(f"å±‚æ¬¡åŒ–æ‹†è§£: {len(hierarchical_intents)} ä¸ªæ„å›¾, åˆ†æ•°: {score4:.3f}")
            except Exception as e:
                logger.warning(f"å±‚æ¬¡åŒ–æ‹†è§£å¤±è´¥: {e}")

        # æ’åºå¹¶è¿”å›top-k
        decomposition_candidates.sort(key=lambda x: x[1], reverse=True)
        top_candidates = decomposition_candidates[:self.config.max_decomposition_methods]

        if self.config.debug_mode:
            logger.info(f"æ„å›¾æ‹†è§£å®Œæˆï¼Œå…±ç”Ÿæˆ {len(top_candidates)} ä¸ªæ–¹æ¡ˆ")

        return top_candidates

    def _generate_detailed_decomposition(self, query: str) -> List[str]:
        """ç”Ÿæˆç»†ç²’åº¦æ‹†è§£"""
        # è·å–åŸºç¡€æ„å›¾
        try:
            base_intents = self.base_retriever._split_query_intent(query)
        except:
            base_intents = [query]

        detailed_intents = []

        for intent in base_intents:
            detailed_intents.append(intent)

            # ä¸ºå¤æ‚æ„å›¾æ·»åŠ ç»†èŠ‚å­æŸ¥è¯¢
            words = intent.split()
            if len(words) > 4:
                # æ·»åŠ å…·ä½“åŒ–çš„å­æŸ¥è¯¢
                if 'features' in intent.lower():
                    detailed_intents.extend([
                        intent.replace('features', 'core features'),
                        intent.replace('features', 'technical specifications'),
                        intent.replace('features', 'user benefits')
                    ])
                elif 'how' in intent.lower():
                    detailed_intents.extend([
                        intent + ' - step by step process',
                        intent + ' - requirements and prerequisites'
                    ])
                else:
                    detailed_intents.append(intent + ' - detailed explanation')

        return list(set(detailed_intents))  # å»é‡

    def _generate_coarse_decomposition(self, base_intents: List[str]) -> List[str]:
        """ç”Ÿæˆç²—ç²’åº¦æ‹†è§£"""
        if len(base_intents) <= 2:
            return base_intents

        # ç®€å•çš„åˆå¹¶ç­–ç•¥ï¼šå°†ç›¸ä¼¼çš„æ„å›¾åˆå¹¶
        coarse_intents = []
        used_indices = set()

        for i, intent in enumerate(base_intents):
            if i in used_indices:
                continue

            # æŸ¥æ‰¾ç›¸ä¼¼æ„å›¾
            similar_indices = []
            intent_words = set(intent.lower().split())

            for j in range(i + 1, len(base_intents)):
                if j in used_indices:
                    continue

                other_words = set(base_intents[j].lower().split())
                overlap = len(intent_words.intersection(other_words))
                union = len(intent_words.union(other_words))

                if union > 0 and overlap / union > 0.5:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    similar_indices.append(j)

            if similar_indices:
                # åˆå¹¶ç›¸ä¼¼æ„å›¾
                all_words = intent_words.copy()
                for idx in similar_indices:
                    all_words.update(base_intents[idx].lower().split())
                combined_intent = f"Combined aspects: {' '.join(sorted(list(all_words))[:8])}"
                coarse_intents.append(combined_intent)
                used_indices.update([i] + similar_indices)
            else:
                coarse_intents.append(intent)
                used_indices.add(i)

        return coarse_intents

    def _generate_hierarchical_decomposition(self, query: str) -> List[str]:
        """ç”Ÿæˆå±‚æ¬¡åŒ–æ‹†è§£"""
        # ä¸»è¦æ„å›¾
        main_intent = f"Primary focus: {query}"

        # æ”¯æ’‘æ„å›¾
        try:
            base_intents = self.base_retriever._split_query_intent(query)
            supporting_intents = [f"Supporting aspect: {intent}" for intent in base_intents[:2]]
        except:
            supporting_intents = [f"Supporting details for: {query}"]

        return [main_intent] + supporting_intents

    def _score_decomposition(self, original_query: str, intents: List[str]) -> float:
        """è¯„ä¼°æ„å›¾æ‹†è§£è´¨é‡"""
        if not intents:
            return 0.0

        try:
            # è¦†ç›–åº¦åˆ†æ•°
            coverage = self._calculate_coverage(original_query, intents)

            # å¤šæ ·æ€§åˆ†æ•°
            diversity = self._calculate_diversity(intents)

            # å¤æ‚åº¦æƒ©ç½š
            complexity_penalty = max(0, 1 - (len(intents) - 3) * 0.1)

            # ç»¼åˆåˆ†æ•°
            total_score = coverage * 0.4 + diversity * 0.4 + complexity_penalty * 0.2

            return max(0.0, min(1.0, total_score))
        except Exception as e:
            logger.warning(f"æ„å›¾æ‹†è§£è¯„åˆ†å¤±è´¥: {e}")
            return 0.5  # é»˜è®¤åˆ†æ•°

    def _calculate_coverage(self, query: str, intents: List[str]) -> float:
        """è®¡ç®—è¦†ç›–åº¦"""
        query_words = set(query.lower().split())
        if not query_words:
            return 0.0

        intent_words = set()
        for intent in intents:
            intent_words.update(intent.lower().split())

        overlap = query_words.intersection(intent_words)
        return len(overlap) / len(query_words)

    def _calculate_diversity(self, intents: List[str]) -> float:
        """è®¡ç®—å¤šæ ·æ€§"""
        if len(intents) <= 1:
            return 1.0

        similarities = []
        for i in range(len(intents)):
            for j in range(i + 1, len(intents)):
                words_i = set(intents[i].lower().split())
                words_j = set(intents[j].lower().split())

                if not words_i or not words_j:
                    continue

                overlap = len(words_i.intersection(words_j))
                union = len(words_i.union(words_j))
                similarity = overlap / union if union > 0 else 0
                similarities.append(similarity)

        if not similarities:
            return 1.0

        avg_similarity = np.mean(similarities)
        return 1 - avg_similarity


class PathEvaluator:
    """è·¯å¾„è¯„ä¼°å™¨"""

    def __init__(self, config: BeamSearchConfig):
        self.config = config

    def evaluate_path(self, path: SearchPath, original_query: str) -> SearchPath:
        """è¯„ä¼°æœç´¢è·¯å¾„"""
        try:
            scores = {}

            # ç›¸å…³æ€§åˆ†æ•°
            scores['relevance'] = self._calculate_relevance_score(path.retrieval_results)

            # å¤šæ ·æ€§åˆ†æ•°
            scores['diversity'] = self._calculate_diversity_score(path.retrieval_results)

            # è¦†ç›–åº¦åˆ†æ•°
            scores['coverage'] = self._calculate_coverage_score(
                path.intent_decomposition, path.retrieval_results
            )

            # æ„å›¾è´¨é‡åˆ†æ•°ï¼ˆä»metadataä¸­è·å–ï¼Œå¦‚æœæœ‰çš„è¯ï¼‰
            scores['intent_quality'] = path.metadata.get('intent_score', 0.5)

            # è®¡ç®—æ€»åˆ†
            scores['total'] = (
                    self.config.relevance_weight * scores['relevance'] +
                    self.config.diversity_weight * scores['diversity'] +
                    self.config.coverage_weight * scores['coverage'] +
                    self.config.intent_quality_weight * scores['intent_quality']
            )

            path.scores = scores

        except Exception as e:
            logger.warning(f"è·¯å¾„è¯„ä¼°å¤±è´¥: {e}")
            path.scores = {'total': 0.0, 'relevance': 0.0, 'diversity': 0.0, 'coverage': 0.0}

        return path

    def _calculate_relevance_score(self, results: List[Dict]) -> float:
        """è®¡ç®—ç›¸å…³æ€§åˆ†æ•°"""
        if not results:
            return 0.0

        scores = [r.get('score', 0) for r in results]
        return np.mean(scores) if scores else 0.0

    def _calculate_diversity_score(self, results: List[Dict]) -> float:
        """è®¡ç®—å¤šæ ·æ€§åˆ†æ•°"""
        if len(results) <= 1:
            return 1.0

        # ç®€åŒ–çš„å¤šæ ·æ€§è®¡ç®—
        texts = [r.get('text', '')[:100] for r in results]
        similarities = []

        for i in range(len(texts)):
            for j in range(i + 1, len(texts)):
                words_i = set(texts[i].lower().split())
                words_j = set(texts[j].lower().split())

                if not words_i or not words_j:
                    continue

                overlap = len(words_i.intersection(words_j))
                union = len(words_i.union(words_j))
                similarity = overlap / union if union > 0 else 0
                similarities.append(similarity)

        if not similarities:
            return 1.0

        avg_similarity = np.mean(similarities)
        return 1 - avg_similarity

    def _calculate_coverage_score(self, intents: List[str], results: List[Dict]) -> float:
        """è®¡ç®—è¦†ç›–åº¦åˆ†æ•°"""
        if not intents or not results:
            return 0.0

        intent_coverage = []

        for intent in intents:
            intent_words = set(intent.lower().split())
            max_coverage = 0

            for result in results:
                result_text = result.get('text', '').lower()
                result_words = set(result_text.split())

                if intent_words and result_words:
                    overlap = len(intent_words.intersection(result_words))
                    coverage = overlap / len(intent_words)
                    max_coverage = max(max_coverage, coverage)

            intent_coverage.append(max_coverage)

        return np.mean(intent_coverage) if intent_coverage else 0.0


class BeamSearchWrapper:
    """Beam SearchåŒ…è£…å™¨ - å³æ’å³ç”¨æ¨¡å—"""

    def _split_query_intent(self, query: str):
        """å§”æ‰˜ç»™åŸºç¡€æ£€ç´¢å™¨çš„æ„å›¾æ‹†åˆ†æ–¹æ³•"""
        return self.base_retriever._split_query_intent(query)

    def __getattr__(self, name):
        """å½“è®¿é—®ä¸å­˜åœ¨çš„å±æ€§æ—¶ï¼Œå°è¯•ä»åŸºç¡€æ£€ç´¢å™¨è·å–"""
        if hasattr(self.base_retriever, name):
            return getattr(self.base_retriever, name)
        raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{name}'")

    def __init__(self,
                 base_retriever,
                 matcher,
                 reranker,
                 enable_beam_search: bool = True,
                 beam_config: Optional[BeamSearchConfig] = None,
                 **kwargs):
        """
        åˆå§‹åŒ–Beam SearchåŒ…è£…å™¨

        Args:
            base_retriever: åŸºç¡€æ£€ç´¢å™¨ (DeepSearch_Beta)
            matcher: å¤šæ¨¡æ€åŒ¹é…å™¨ (MultimodalMatcher)
            reranker: é‡æ’åºå™¨ (FlagReranker)
            enable_beam_search: æ˜¯å¦å¯ç”¨Beam Search
            beam_config: Beam Searché…ç½®
            **kwargs: å…¶ä»–é…ç½®å‚æ•°ï¼Œä¼šè¢«åº”ç”¨åˆ°beam_config
        """
        self.base_retriever = base_retriever
        self.matcher = matcher
        self.reranker = reranker
        self.enable_beam_search = enable_beam_search

        # åˆå§‹åŒ–é…ç½®
        if beam_config is None:
            beam_config = BeamSearchConfig()

        # åº”ç”¨é¢å¤–çš„é…ç½®å‚æ•°
        for key, value in kwargs.items():
            if hasattr(beam_config, key):
                setattr(beam_config, key, value)

        self.config = beam_config
        self.config.enable = enable_beam_search  # ç¡®ä¿é…ç½®ä¸å‚æ•°ä¸€è‡´

        # åˆå§‹åŒ–ç»„ä»¶
        if self.enable_beam_search:
            self.decomposer = IntentDecomposer(base_retriever, self.config)
            self.evaluator = PathEvaluator(self.config)
            self.path_counter = 0

        if self.config.debug_mode:
            logger.info(f"BeamSearchWrapperåˆå§‹åŒ–å®Œæˆï¼ŒBeam Search: {'å¯ç”¨' if enable_beam_search else 'ç¦ç”¨'}")

    def search_retrieval(self, data: Dict[str, Any], retriever=None, **kwargs) -> List[Dict[str, Any]]:
        """
        ä¸»è¦çš„æ£€ç´¢æ¥å£ - ä¸åŸæœ‰æ¥å£å®Œå…¨å…¼å®¹

        Args:
            data: åŒ…å«queryå’Œdocumentsçš„æ•°æ®å­—å…¸
            retriever: æ£€ç´¢å™¨ï¼ˆå…¼å®¹æ€§å‚æ•°ï¼‰
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        if not self.enable_beam_search or not self.config.enable:
            # å¦‚æœæœªå¯ç”¨Beam Searchï¼Œç›´æ¥ä½¿ç”¨åŸæœ‰æ–¹æ³•
            if self.config.debug_mode:
                logger.info("ğŸ”„ ä½¿ç”¨ä¼ ç»Ÿæ£€ç´¢æ–¹æ³•")
            return self.base_retriever.search_retrieval(data, retriever=retriever or self.matcher)

        # ä½¿ç”¨Beam Search
        if self.config.debug_mode:
            logger.info("ğŸš€ ä½¿ç”¨Beam Searchæ£€ç´¢æ–¹æ³•")

        return self._beam_search_retrieval(data)

    def _beam_search_retrieval(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒBeam Searchæ£€ç´¢"""
        query = data['query']
        documents = data['documents']

        try:
            # ç¬¬ä¸€é˜¶æ®µï¼šæ„å›¾æ‹†è§£
            intent_candidates = self.decomposer.decompose_with_beam_search(query)

            if self.config.log_paths:
                logger.info(f"ç”Ÿæˆäº† {len(intent_candidates)} ä¸ªæ„å›¾æ‹†è§£æ–¹æ¡ˆ")

            # ç¬¬äºŒé˜¶æ®µï¼šè·¯å¾„æ„å»ºå’Œè¯„ä¼°
            all_paths = []

            for intents, intent_score, method in intent_candidates:
                # æ‰§è¡Œæ£€ç´¢
                path_results = self._execute_retrieval_for_intents(intents, documents)

                # åˆ›å»ºè·¯å¾„
                path = SearchPath(
                    path_id=f"path_{self.path_counter}_{method}",
                    intent_decomposition=intents,
                    retrieval_results=path_results,
                    metadata={'intent_score': intent_score, 'method': method}
                )
                self.path_counter += 1

                # è¯„ä¼°è·¯å¾„
                path = self.evaluator.evaluate_path(path, query)

                if path.total_score >= self.config.min_path_score:
                    all_paths.append(path)

                if self.config.log_paths:
                    logger.info(f"è·¯å¾„ {path.path_id}: åˆ†æ•°={path.total_score:.3f}, æ–¹æ³•={method}")

            # ç¬¬ä¸‰é˜¶æ®µï¼šè·¯å¾„é€‰æ‹©
            selected_paths = self._select_diverse_paths(all_paths)

            # ç¬¬å››é˜¶æ®µï¼šç»“æœåˆæˆ
            final_results = self._synthesize_final_results(selected_paths, query)

            if self.config.debug_mode:
                logger.info(f"Beam Searchæ£€ç´¢å®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")

            return final_results

        except Exception as e:
            logger.error(f"Beam Searchæ£€ç´¢å¤±è´¥: {e}")
            # å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
            logger.info("å›é€€åˆ°ä¼ ç»Ÿæ£€ç´¢æ–¹æ³•")
            return self.base_retriever.search_retrieval(data, retriever=self.matcher)

    def _execute_retrieval_for_intents(self, intents: List[str], documents: List[Dict]) -> List[Dict]:
        """ä¸ºæ„å›¾åˆ—è¡¨æ‰§è¡Œæ£€ç´¢"""
        all_results = []

        for intent in intents:
            try:
                # æ‰§è¡Œå•ä¸ªæ„å›¾çš„æ£€ç´¢
                intent_results = self.matcher.retrieve(intent, documents)

                # æ ‡è®°æ¥æºæ„å›¾
                for result in intent_results:
                    result['source_intent'] = intent

                all_results.extend(intent_results)

            except Exception as e:
                logger.warning(f"æ„å›¾æ£€ç´¢å¤±è´¥ '{intent}': {e}")
                continue

        # å»é‡
        unique_results = self._deduplicate_results(all_results)

        # é‡æ’åº
        if self.config.enable_result_reranking and unique_results:
            unique_results = self._rerank_results(intents[0] if intents else "", unique_results)

        return unique_results

    def _select_diverse_paths(self, paths: List[SearchPath]) -> List[SearchPath]:
        """é€‰æ‹©å¤šæ ·åŒ–çš„è·¯å¾„"""
        if not paths:
            return []

        # æŒ‰åˆ†æ•°æ’åº
        paths.sort(reverse=True)

        selected = []

        for path in paths:
            if len(selected) >= self.config.beam_width:
                break

            # æ£€æŸ¥å¤šæ ·æ€§
            if self._is_diverse_enough(path, selected):
                selected.append(path)

        # å¦‚æœé€‰æ‹©çš„è·¯å¾„ä¸å¤Ÿï¼Œè¡¥å……é«˜åˆ†è·¯å¾„
        while len(selected) < self.config.beam_width and len(selected) < len(paths):
            for path in paths:
                if path not in selected:
                    selected.append(path)
                    break

        return selected

    def _is_diverse_enough(self, new_path: SearchPath, selected_paths: List[SearchPath]) -> bool:
        """æ£€æŸ¥è·¯å¾„æ˜¯å¦è¶³å¤Ÿå¤šæ ·åŒ–"""
        if not selected_paths:
            return True

        for selected_path in selected_paths:
            similarity = self._calculate_path_similarity(new_path, selected_path)
            if similarity > self.config.similarity_threshold:
                return False

        return True

    def _calculate_path_similarity(self, path1: SearchPath, path2: SearchPath) -> float:
        """è®¡ç®—è·¯å¾„ç›¸ä¼¼åº¦"""
        # æ„å›¾ç›¸ä¼¼åº¦
        intents1_text = ' '.join(path1.intent_decomposition).lower()
        intents2_text = ' '.join(path2.intent_decomposition).lower()

        words1 = set(intents1_text.split())
        words2 = set(intents2_text.split())

        if not words1 or not words2:
            return 0.0

        overlap = len(words1.intersection(words2))
        union = len(words1.union(words2))

        return overlap / union if union > 0 else 0.0

    def _synthesize_final_results(self, paths: List[SearchPath], original_query: str) -> List[Dict]:
        """åˆæˆæœ€ç»ˆç»“æœ"""
        all_results = []

        # æ”¶é›†æ‰€æœ‰è·¯å¾„çš„ç»“æœ
        for path in paths:
            for result in path.retrieval_results:
                result['path_id'] = path.path_id
                result['path_score'] = path.total_score
                all_results.append(result)

        # å»é‡
        unique_results = self._deduplicate_results(all_results)

        # é‡æ–°è®¡ç®—åˆ†æ•°ï¼šç»“åˆè·¯å¾„åˆ†æ•°å’Œæ£€ç´¢åˆ†æ•°
        for result in unique_results:
            original_score = result.get('score', 0)
            path_score = result.get('path_score', 0)
            # åŠ æƒç»„åˆ
            result['final_score'] = 0.7 * original_score + 0.3 * path_score

        # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
        unique_results.sort(key=lambda x: x.get('final_score', 0), reverse=True)

        # é™åˆ¶æœ€ç»ˆç»“æœæ•°é‡
        final_count = min(len(unique_results), self.config.max_final_results)
        return unique_results[:final_count]

    def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """å»é‡ç»“æœ"""
        seen_texts = set()
        unique_results = []

        for result in results:
            text = result.get('text', '')
            text_key = text[:100] if text else ''  # ä½¿ç”¨å‰100å­—ç¬¦ä½œä¸ºæ ‡è¯†

            if text_key not in seen_texts:
                seen_texts.add(text_key)
                unique_results.append(result)

        return unique_results

    def _rerank_results(self, query: str, results: List[Dict]) -> List[Dict]:
        """é‡æ’åºç»“æœ"""
        if not results or not query or not self.reranker:
            return results

        try:
            pairs = [[query, result.get('text', '')] for result in results]
            rerank_scores = self.reranker.compute_score(pairs, normalize=True)

            for i, result in enumerate(results):
                if i < len(rerank_scores):
                    result['rerank_score'] = rerank_scores[i]
                    # æ›´æ–°åˆ†æ•°
                    original_score = result.get('score', 0)
                    result['score'] = 0.6 * rerank_scores[i] + 0.4 * original_score

            results.sort(key=lambda x: x.get('score', 0), reverse=True)

        except Exception as e:
            logger.warning(f"é‡æ’åºå¤±è´¥: {e}")

        return results

    # æä¾›ä¾¿æ·çš„é…ç½®æ–¹æ³•
    def enable_beam_search_mode(self, enable: bool = True):
        """å¯ç”¨/ç¦ç”¨Beam Searchæ¨¡å¼"""
        self.enable_beam_search = enable
        self.config.enable = enable

        if self.config.debug_mode:
            logger.info(f"Beam Searchæ¨¡å¼å·²{'å¯ç”¨' if enable else 'ç¦ç”¨'}")

    def set_beam_width(self, width: int):
        """è®¾ç½®beamå®½åº¦"""
        self.config.beam_width = width

        if self.config.debug_mode:
            logger.info(f"Beamå®½åº¦è®¾ç½®ä¸º: {width}")

    def set_debug_mode(self, debug: bool = True):
        """è®¾ç½®è°ƒè¯•æ¨¡å¼"""
        self.config.debug_mode = debug

        if debug:
            logger.info("è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")

    def get_config(self) -> BeamSearchConfig:
        """è·å–å½“å‰é…ç½®"""
        return self.config

    def update_config(self, **kwargs):
        """æ›´æ–°é…ç½®"""
        for key, value in kwargs.items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)
                if self.config.debug_mode:
                    logger.info(f"é…ç½®æ›´æ–°: {key} = {value}")


# ä¾¿æ·çš„å·¥å‚å‡½æ•°
def create_beam_search_retriever(base_retriever, matcher, reranker,
                                 enable_beam_search: bool = True,
                                 beam_width: int = 3,
                                 debug_mode: bool = False,
                                 **config_kwargs) -> BeamSearchWrapper:
    """
    ä¾¿æ·çš„å·¥å‚å‡½æ•°ï¼Œç”¨äºåˆ›å»ºBeam Searchæ£€ç´¢å™¨

    Args:
        base_retriever: åŸºç¡€æ£€ç´¢å™¨
        matcher: åŒ¹é…å™¨
        reranker: é‡æ’åºå™¨
        enable_beam_search: æ˜¯å¦å¯ç”¨Beam Search
        beam_width: beamå®½åº¦
        debug_mode: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
        **config_kwargs: å…¶ä»–é…ç½®å‚æ•°

    Returns:
        BeamSearchWrapperå®ä¾‹
    """
    config = BeamSearchConfig(
        enable=enable_beam_search,
        beam_width=beam_width,
        debug_mode=debug_mode,
        **config_kwargs
    )

    return BeamSearchWrapper(
        base_retriever=base_retriever,
        matcher=matcher,
        reranker=reranker,
        enable_beam_search=enable_beam_search,
        beam_config=config
    )


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # è¿™é‡Œå¯ä»¥æ”¾ä¸€äº›ç®€å•çš„æµ‹è¯•ä»£ç 
    print("ğŸš€ Beam Searchæ¨¡å—åŠ è½½æˆåŠŸï¼")
    print("ä½¿ç”¨æ–¹æ³•:")
    print("from beam_search_module import BeamSearchWrapper, create_beam_search_retriever")
    print()
    print("# æ–¹å¼1: ç›´æ¥åŒ…è£…")
    print("wrapped_retriever = BeamSearchWrapper(base_retriever, matcher, reranker, enable_beam_search=True)")
    print()
    print("# æ–¹å¼2: ä½¿ç”¨å·¥å‚å‡½æ•°")
    print("wrapped_retriever = create_beam_search_retriever(base_retriever, matcher, reranker, beam_width=3)")
    print()
    print("# ä½¿ç”¨ï¼ˆä¸åŸæ¥å£å®Œå…¨å…¼å®¹ï¼‰")
    print("results = wrapped_retriever.search_retrieval(data, retriever=matcher)")