import os
import sys
import json
import time
import argparse
from pathlib import Path
from tqdm import tqdm
import numpy as np
from dotenv import load_dotenv
import torch
from pdf2image import convert_from_path
from PIL import Image
import logging
from collections import defaultdict, Counter

# åˆ›å»ºæ—¥å¿—
log_dir = Path("./log")
log_dir.mkdir(exist_ok=True)
log_file = log_dir / "text_only_baseline_test.log"

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(str(log_file), mode='a', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

logger.info("=== çº¯æ–‡æœ¬Baselineå¤šæ„å›¾æ£€ç´¢æµ‹è¯•å¼€å§‹ï¼ˆ50æ¡æ•°æ®ï¼‰===")

# æ·»åŠ è·¯å¾„
sys.path.append("multimodal-RAG/DeepRAG_Multimodal/deep_retrieve")

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv("/root/autodl-tmp/multimodal-RAG/multimodal-RAG/DeepRAG_Multimodal/configs/.env")

# å¯¼å…¥å¿…è¦çš„åº“
from DeepRAG_Multimodal.deep_retrieve.ming.deepsearch_optimize_ming import DeepSearch_Beta


class TextOnlyMultiIntentTester:
    """çº¯æ–‡æœ¬å¤šæ„å›¾æ£€ç´¢æµ‹è¯•ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.config = self.load_config()
        os.makedirs(self.config['results_dir'], exist_ok=True)
        self.verify_model_files()
        self.setup_models()

    def load_config(self):
        """åŠ è½½é…ç½®"""
        config = {
            # è·¯å¾„é…ç½®
            'test_data_path': '/root/autodl-tmp/multimodal-RAG/multimodal-RAG/DeepRAG_Multimodal/picked_LongDoc/selected_LongDocURL_public_with_subtask_category.jsonl',
            'pdf_base_dir': '/root/autodl-tmp/multimodal-RAG/multimodal-RAG/DeepRAG_Multimodal/picked_LongDoc',
            'results_dir': './test_results',

            # ğŸ”¥ é‡‡æ ·é…ç½® - æ”¹ä¸º50æ¡æ•°æ®
            'sample_size': 50,
            'debug': False,  # å…³é—­è°ƒè¯•æ¨¡å¼ä»¥å¤„ç†æ›´å¤šæ•°æ®

            # æ£€ç´¢é…ç½®
            'max_iterations': 2,
            'embedding_topk': 12,
            'rerank_topk': 5,
            'text_weight': 1.0,  # çº¯æ–‡æœ¬æ¨¡å¼
            'image_weight': 0.0,

            # æ¨¡å‹é…ç½® - åªéœ€è¦æ–‡æœ¬æ¨¡å‹
            'bge_model_name': "/root/autodl-tmp/multimodal-RAG/hf_models/bge-large-en-v1.5",
            'reranker_model_name': "/root/autodl-tmp/multimodal-RAG/hf_models/bge-reranker-large",

            'device': 'cuda:0',
            'batch_size': 4,
            'ocr_method': 'pytesseract',
        }
        return config

    def verify_model_files(self):
        """éªŒè¯æ¨¡å‹æ–‡ä»¶"""
        model_paths = [
            self.config['bge_model_name'],
            self.config['reranker_model_name']
        ]

        for model_path in model_paths:
            if not os.path.exists(model_path):
                logger.error(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
                raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            else:
                logger.info(f"âœ… æ¨¡å‹è·¯å¾„éªŒè¯æˆåŠŸ: {model_path}")

    def setup_models(self):
        """åˆå§‹åŒ–æ£€ç´¢æ¨¡å‹"""
        logger.info("ğŸš€ åˆå§‹åŒ–çº¯æ–‡æœ¬å¤šæ„å›¾æ£€ç´¢æ¨¡å‹...")
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        logger.info(f"ğŸ® ä½¿ç”¨è®¾å¤‡: {device}")

        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated() / (1024 ** 3)
            logger.info(f"ğŸ§¹ åˆå§‹GPUå†…å­˜ä½¿ç”¨: {initial_memory:.2f}GB")

        try:
            # åˆå§‹åŒ–é‡æ’åºå™¨
            from FlagEmbedding import FlagReranker

            logger.info("â³ åˆå§‹åŒ–é‡æ’åºå™¨...")
            self.reranker = FlagReranker(
                model_name_or_path=self.config['reranker_model_name'],
                use_fp16=True,
                device=device,
                local_files_only=True
            )
            logger.info("âœ… é‡æ’åºå™¨åˆå§‹åŒ–æˆåŠŸ")

            # åˆå§‹åŒ–çº¯æ–‡æœ¬åŒ¹é…å™¨
            logger.info("â³ åˆå§‹åŒ–çº¯æ–‡æœ¬åŒ¹é…å™¨...")
            self.text_matcher = TextOnlyMatcher(
                bge_model_path=self.config['bge_model_name'],
                device=device,
                topk=self.config['rerank_topk']
            )
            logger.info("âœ… çº¯æ–‡æœ¬åŒ¹é…å™¨åˆå§‹åŒ–æˆåŠŸ")

            # åˆå§‹åŒ– DeepSearch_Beta
            logger.info("â³ åˆå§‹åŒ–å¤šæ„å›¾æ£€ç´¢å™¨...")
            self.multi_intent_search = DeepSearch_Beta(
                max_iterations=self.config['max_iterations'],
                reranker=self.reranker,
                params={
                    "embedding_topk": self.config['embedding_topk'],
                    "rerank_topk": self.config['rerank_topk'],
                    "text_weight": self.config['text_weight'],
                    "image_weight": self.config['image_weight']
                }
            )
            logger.info("âœ… å¤šæ„å›¾æ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸ")

            if torch.cuda.is_available():
                final_memory = torch.cuda.memory_allocated() / (1024 ** 3)
                logger.info(f"ğŸ“Š æœ€ç»ˆGPUå†…å­˜ä½¿ç”¨: {final_memory:.2f}GB")

            logger.info("âœ… çº¯æ–‡æœ¬æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            raise

    def load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        allowed_doc_nos = [
            '4046173.pdf', '4176503.pdf', '4057524.pdf', '4064501.pdf', '4057121.pdf', '4174854.pdf',
            '4148165.pdf', '4129570.pdf', '4010333.pdf', '4147727.pdf', '4066338.pdf', '4031704.pdf',
            '4050613.pdf', '4072260.pdf', '4091919.pdf', '4094684.pdf', '4063393.pdf', '4132494.pdf',
            '4185438.pdf', '4129670.pdf', '4138347.pdf', '4190947.pdf', '4100212.pdf', '4173940.pdf',
            '4069930.pdf', '4174181.pdf', '4027862.pdf', '4012567.pdf', '4145761.pdf', '4078345.pdf',
            '4061601.pdf', '4170122.pdf', '4077673.pdf', '4107960.pdf', '4005877.pdf', '4196005.pdf',
            '4126467.pdf', '4088173.pdf', '4106951.pdf', '4086173.pdf', '4072232.pdf', '4111230.pdf',
            '4057714.pdf'
        ]

        logger.info(f"ğŸ“š åŠ è½½æµ‹è¯•æ•°æ®: {self.config['test_data_path']}")
        test_data = []

        with open(self.config['test_data_path'], 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    item = json.loads(line)
                    if item.get("pdf_path") in allowed_doc_nos:
                        test_data.append(item)

        # ğŸ”¥ é‡‡æ ·50æ¡æ•°æ®
        if self.config['sample_size'] > 0 and len(test_data) > self.config['sample_size']:
            np.random.seed(42)  # å›ºå®šéšæœºç§å­ä¿è¯ç»“æœå¯å¤ç°
            test_data = np.random.choice(test_data, self.config['sample_size'], replace=False).tolist()

        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(test_data)} æ¡æµ‹è¯•æ•°æ®")
        return test_data

    def process_single_document(self, doc_data):
        """å¤„ç†å•ä¸ªæ–‡æ¡£ - ä»…æå–æ–‡æœ¬"""
        documents = []
        pdf_path = os.path.join(self.config['pdf_base_dir'], doc_data["pdf_path"])

        # è·å–OCRç»“æœ
        ocr_file = os.path.join(
            self.config['pdf_base_dir'],
            f"{self.config['ocr_method']}_save",
            f"{os.path.basename(doc_data['pdf_path']).replace('.pdf', '.json')}"
        )

        if os.path.exists(ocr_file):
            with open(ocr_file, 'r', encoding='utf-8') as f:
                loaded_data = json.load(f)
            logger.info(f"ğŸ“– æˆåŠŸè¯»å–OCRæ–‡ä»¶: {ocr_file}")
        else:
            logger.warning(f"âš ï¸ æ‰¾ä¸åˆ°OCRæ–‡ä»¶: {ocr_file}")
            return []

        # åªå¤„ç†æ–‡æœ¬ï¼Œä¸å¤„ç†å›¾åƒ
        page_keys = list(loaded_data.keys())
        for idx, page_key in enumerate(page_keys):
            page_text = loaded_data[page_key]
            if not page_text.strip():
                page_text = f"ç¬¬{idx + 1}é¡µå†…å®¹"

            documents.append({
                "text": page_text,
                "metadata": {
                    "page_index": idx + 1,
                    "pdf_path": doc_data.get("pdf_path", "")
                }
            })

        logger.info(f"ğŸ“‘ æˆåŠŸåˆ›å»º {len(documents)} ä¸ªæ–‡æ¡£å¯¹è±¡ï¼ˆçº¯æ–‡æœ¬ï¼‰")

        # æ–‡æœ¬è´¨é‡æ£€æŸ¥
        total_text_length = sum(len(doc['text']) for doc in documents)
        logger.info(f"ğŸ“ æ€»æ–‡æœ¬é•¿åº¦: {total_text_length} å­—ç¬¦")

        return documents

    def test_text_only_retrieval(self):
        """æµ‹è¯•çº¯æ–‡æœ¬å¤šæ„å›¾æ£€ç´¢"""
        logger.info("ğŸ¯ å¼€å§‹çº¯æ–‡æœ¬å¤šæ„å›¾æ£€ç´¢æµ‹è¯•...")
        test_data = self.load_test_data()
        results = []

        # ğŸ”¥ æ·»åŠ è¿›åº¦è·Ÿè¸ª
        success_count = 0
        error_count = 0

        for idx, doc_data in enumerate(tqdm(test_data, desc="çº¯æ–‡æœ¬æ£€ç´¢æµ‹è¯•")):
            try:
                query = doc_data.get("question", "")
                evidence_pages = doc_data.get("evidence_pages", [])

                logger.info(f"\n{'=' * 60}")
                logger.info(f"ğŸ” å¤„ç†æ–‡æ¡£ {idx + 1}/{len(test_data)}: {doc_data.get('pdf_path', 'Unknown')}")
                logger.info(f"â“ æŸ¥è¯¢: {query}")
                logger.info(f"ğŸ“‹ è¯æ®é¡µé¢: {evidence_pages}")

                document_pages = self.process_single_document(doc_data)
                if not document_pages:
                    logger.warning(f"âš ï¸ è·³è¿‡æ–‡æ¡£: æ— æœ‰æ•ˆå†…å®¹")
                    error_count += 1
                    continue

                start_time = time.time()
                logger.info("ğŸ“„ ä½¿ç”¨å¤šæ„å›¾æ‹†è§£ + çº¯æ–‡æœ¬æ£€ç´¢")

                data = {"query": query, "documents": document_pages}
                retrieval_results = self.multi_intent_search.search_retrieval(data, retriever=self.text_matcher)

                elapsed_time = time.time() - start_time

                # å¤„ç†ç»“æœ
                retrieved_pages = set()
                processed_results = []

                for result in retrieval_results:
                    text = result.get("text", "")
                    score = result.get("score", 0)
                    metadata = result.get("metadata", {})
                    page_index = result.get("page", metadata.get("page_index", None))

                    if page_index is not None:
                        retrieved_pages.add(page_index)

                    processed_results.append({
                        "text": text,
                        "score": score,
                        "page": page_index,
                        "metadata": metadata
                    })

                # è¯„ä¼°ç»“æœ
                evidence_set = set(evidence_pages)
                correct_pages = evidence_set.intersection(retrieved_pages)

                recall = len(correct_pages) / len(evidence_set) if evidence_set else 0
                precision = len(correct_pages) / len(retrieved_pages) if retrieved_pages else 0
                f1 = 2 * recall * precision / (recall + precision) if (recall + precision) > 0 else 0

                retrieval_scores = [r.get('score', 0) for r in processed_results]

                is_success = len(correct_pages) == len(evidence_set)
                if is_success:
                    success_count += 1

                logger.info(f"â±ï¸ æ£€ç´¢è€—æ—¶: {elapsed_time:.2f}ç§’")
                logger.info(f"ğŸ¯ æ£€ç´¢åˆ°é¡µé¢: {sorted(list(retrieved_pages))}")
                logger.info(f"âœ… æ­£ç¡®é¡µé¢: {sorted(list(correct_pages))}")
                logger.info(f"ğŸ“Š æ£€ç´¢åˆ†æ•°: {retrieval_scores[:5]}")
                logger.info(f"ğŸ“ˆ å¬å›ç‡: {recall:.4f}")
                logger.info(f"ğŸ“ˆ ç²¾ç¡®ç‡: {precision:.4f}")
                logger.info(f"ğŸ“ˆ F1å€¼: {f1:.4f}")
                logger.info(f"ğŸ¯ æ˜¯å¦æˆåŠŸ: {'âœ… æ˜¯' if is_success else 'âŒ å¦'}")

                # è®°å½•ç»“æœ
                result = {
                    "doc_id": doc_data.get("doc_no", ""),
                    "pdf_path": doc_data.get("pdf_path", ""),
                    "query": query,
                    "evidence_pages": evidence_pages,
                    "task_tag": doc_data.get("task_tag", ""),
                    "subTask": doc_data.get("subTask", []),
                    "retrieved_pages": list(retrieved_pages),
                    "correct_pages": list(correct_pages),
                    "recall": recall,
                    "precision": precision,
                    "f1": f1,
                    "retrieval_time": elapsed_time,
                    "retrieval_scores": retrieval_scores[:10],
                    "success": is_success,
                    "strategy": "text_only_baseline",
                    "num_evidence_pages": len(evidence_pages),
                    "num_retrieved_pages": len(retrieved_pages),
                    "num_correct_pages": len(correct_pages)
                }

                results.append(result)

            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ–‡æ¡£ {doc_data.get('pdf_path', '')} æ—¶å‡ºé”™: {str(e)}")
                error_count += 1
                import traceback
                traceback.print_exc()

        # ğŸ”¥ å®æ—¶è¿›åº¦æ±‡æ€»
        logger.info(f"\nğŸ“Š æµ‹è¯•è¿›åº¦æ±‡æ€»:")
        logger.info(f"   æ€»æµ‹è¯•æ–‡æ¡£: {len(test_data)}")
        logger.info(f"   æˆåŠŸå¤„ç†: {len(results)}")
        logger.info(f"   æˆåŠŸæ£€ç´¢: {success_count}")
        logger.info(f"   å¤„ç†é”™è¯¯: {error_count}")
        logger.info(f"   æˆåŠŸç‡: {success_count / len(results) * 100:.1f}%" if results else "0%")

        # ä¿å­˜å’Œåˆ†æç»“æœ
        result_file = os.path.join(self.config['results_dir'], 'text_only_baseline_results_50.json')
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        # ğŸ”¥ ç”Ÿæˆè¯¦ç»†æ±‡æ€»æŠ¥å‘Š
        self.analyze_results(results)
        self.generate_summary_report(results)

        logger.info(f"ğŸ‰ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
        return results

    def analyze_results(self, results):
        """åˆ†ææµ‹è¯•ç»“æœ"""
        if not results:
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„ç»“æœè¿›è¡Œåˆ†æ")
            return

        recalls = [r["recall"] for r in results]
        precisions = [r["precision"] for r in results]
        f1s = [r["f1"] for r in results]
        times = [r["retrieval_time"] for r in results]
        success_count = sum(1 for r in results if r["success"])

        all_scores = []
        for r in results:
            all_scores.extend(r["retrieval_scores"])
        non_zero_scores = [s for s in all_scores if s > 0]

        avg_recall = np.mean(recalls)
        avg_precision = np.mean(precisions)
        avg_f1 = np.mean(f1s)
        avg_time = np.mean(times)
        success_rate = success_count / len(results) * 100

        logger.info(f"\n{'=' * 80}")
        logger.info(f"ğŸ“Š çº¯æ–‡æœ¬Baselineå¤šæ„å›¾æ£€ç´¢æ€§èƒ½åˆ†æï¼ˆ50æ¡æ•°æ®ï¼‰")
        logger.info(f"{'=' * 80}")
        logger.info(f"ğŸ“‹ æµ‹è¯•æ–‡æ¡£æ•°: {len(results)}")
        logger.info(f"ğŸ“ˆ å¹³å‡å¬å›ç‡: {avg_recall:.4f}")
        logger.info(f"ğŸ“ˆ å¹³å‡ç²¾ç¡®ç‡: {avg_precision:.4f}")
        logger.info(f"ğŸ“ˆ å¹³å‡F1å€¼: {avg_f1:.4f}")
        logger.info(f"â±ï¸ å¹³å‡æ£€ç´¢æ—¶é—´: {avg_time:.2f}ç§’")
        logger.info(f"ğŸ¯ æˆåŠŸç‡: {success_rate:.2f}% ({success_count}/{len(results)})")

        # ğŸ”¥ è¯¦ç»†ç»Ÿè®¡åˆ†æ
        logger.info(f"\nğŸ“Š è¯¦ç»†ç»Ÿè®¡åˆ†æ:")
        logger.info(f"   å¬å›ç‡ - æœ€é«˜: {max(recalls):.4f}, æœ€ä½: {min(recalls):.4f}, æ ‡å‡†å·®: {np.std(recalls):.4f}")
        logger.info(
            f"   ç²¾ç¡®ç‡ - æœ€é«˜: {max(precisions):.4f}, æœ€ä½: {min(precisions):.4f}, æ ‡å‡†å·®: {np.std(precisions):.4f}")
        logger.info(f"   F1å€¼   - æœ€é«˜: {max(f1s):.4f}, æœ€ä½: {min(f1s):.4f}, æ ‡å‡†å·®: {np.std(f1s):.4f}")
        logger.info(f"   æ£€ç´¢æ—¶é—´ - æœ€é•¿: {max(times):.2f}ç§’, æœ€çŸ­: {min(times):.2f}ç§’, æ ‡å‡†å·®: {np.std(times):.2f}ç§’")

        logger.info(f"\nğŸ“Š åˆ†æ•°è´¨é‡åˆ†æ:")
        logger.info(f"   æ€»åˆ†æ•°æ•°é‡: {len(all_scores)}")
        logger.info(f"   éé›¶åˆ†æ•°æ•°é‡: {len(non_zero_scores)}")
        if non_zero_scores:
            logger.info(f"   éé›¶åˆ†æ•°æ¯”ä¾‹: {len(non_zero_scores) / len(all_scores) * 100:.1f}%")
            logger.info(f"   æœ€é«˜åˆ†æ•°: {max(non_zero_scores):.4f}")
            logger.info(f"   æœ€ä½åˆ†æ•°: {min(non_zero_scores):.4f}")
            logger.info(f"   å¹³å‡éé›¶åˆ†æ•°: {np.mean(non_zero_scores):.4f}")
            logger.info(f"âœ… æ£€ç´¢åˆ†æ•°æ­£å¸¸")
        else:
            logger.warning(f"âš ï¸ æ‰€æœ‰æ£€ç´¢åˆ†æ•°éƒ½ä¸º0ï¼Œè¯·æ£€æŸ¥é…ç½®ï¼")

        logger.info(f"{'=' * 80}")

    def generate_summary_report(self, results):
        """ğŸ”¥ ç”Ÿæˆè¯¦ç»†æ±‡æ€»æŠ¥å‘Š"""
        if not results:
            return

        logger.info(f"\nğŸ”¥ ç”Ÿæˆè¯¦ç»†æ±‡æ€»æŠ¥å‘Š...")

        # æŒ‰ä»»åŠ¡ç±»å‹åˆ†æ
        task_stats = defaultdict(
            lambda: {"count": 0, "success": 0, "recalls": [], "precisions": [], "f1s": [], "times": []})

        for r in results:
            task_tag = r.get("task_tag", "Unknown")
            task_stats[task_tag]["count"] += 1
            task_stats[task_tag]["recalls"].append(r["recall"])
            task_stats[task_tag]["precisions"].append(r["precision"])
            task_stats[task_tag]["f1s"].append(r["f1"])
            task_stats[task_tag]["times"].append(r["retrieval_time"])
            if r["success"]:
                task_stats[task_tag]["success"] += 1

        # æŒ‰è¯æ®é¡µé¢æ•°é‡åˆ†æ
        page_stats = defaultdict(lambda: {"count": 0, "success": 0, "recalls": [], "precisions": [], "f1s": []})

        for r in results:
            num_pages = r["num_evidence_pages"]
            page_stats[num_pages]["count"] += 1
            page_stats[num_pages]["recalls"].append(r["recall"])
            page_stats[num_pages]["precisions"].append(r["precision"])
            page_stats[num_pages]["f1s"].append(r["f1"])
            if r["success"]:
                page_stats[num_pages]["success"] += 1

        # æ€§èƒ½åˆ†å¸ƒåˆ†æ
        f1_ranges = {
            "ä¼˜ç§€ (F1â‰¥0.8)": [r for r in results if r["f1"] >= 0.8],
            "è‰¯å¥½ (0.6â‰¤F1<0.8)": [r for r in results if 0.6 <= r["f1"] < 0.8],
            "ä¸€èˆ¬ (0.4â‰¤F1<0.6)": [r for r in results if 0.4 <= r["f1"] < 0.6],
            "è¾ƒå·® (F1<0.4)": [r for r in results if r["f1"] < 0.4]
        }

        # è¾“å‡ºæ±‡æ€»æŠ¥å‘Š
        logger.info(f"\n{'ğŸ”¥' * 20} è¯¦ç»†æ±‡æ€»æŠ¥å‘Š {'ğŸ”¥' * 20}")

        # 1. æŒ‰ä»»åŠ¡ç±»å‹åˆ†æ
        if len(task_stats) > 1:
            logger.info(f"\nğŸ“‹ æŒ‰ä»»åŠ¡ç±»å‹åˆ†æ:")
            for task_tag, stats in sorted(task_stats.items()):
                if stats["count"] > 0:
                    avg_f1 = np.mean(stats["f1s"])
                    success_rate = stats["success"] / stats["count"] * 100
                    avg_time = np.mean(stats["times"])
                    logger.info(f"   {task_tag}:")
                    logger.info(f"     æ ·æœ¬æ•°: {stats['count']}, æˆåŠŸç‡: {success_rate:.1f}%")
                    logger.info(f"     å¹³å‡F1: {avg_f1:.4f}, å¹³å‡æ—¶é—´: {avg_time:.2f}ç§’")

        # 2. æŒ‰è¯æ®é¡µé¢æ•°é‡åˆ†æ
        logger.info(f"\nğŸ“„ æŒ‰è¯æ®é¡µé¢æ•°é‡åˆ†æ:")
        for num_pages in sorted(page_stats.keys()):
            stats = page_stats[num_pages]
            if stats["count"] > 0:
                avg_f1 = np.mean(stats["f1s"])
                success_rate = stats["success"] / stats["count"] * 100
                logger.info(
                    f"   {num_pages}é¡µè¯æ®: {stats['count']}ä¸ªæ ·æœ¬, æˆåŠŸç‡: {success_rate:.1f}%, å¹³å‡F1: {avg_f1:.4f}")

        # 3. æ€§èƒ½åˆ†å¸ƒåˆ†æ
        logger.info(f"\nğŸ“Š æ€§èƒ½åˆ†å¸ƒåˆ†æ:")
        for range_name, range_results in f1_ranges.items():
            count = len(range_results)
            percentage = count / len(results) * 100
            logger.info(f"   {range_name}: {count}ä¸ªæ ·æœ¬ ({percentage:.1f}%)")

        # 4. å¤±è´¥æ¡ˆä¾‹åˆ†æ
        failed_cases = [r for r in results if not r["success"]]
        if failed_cases:
            logger.info(f"\nâŒ å¤±è´¥æ¡ˆä¾‹åˆ†æ ({len(failed_cases)}ä¸ª):")

            # æŒ‰å¤±è´¥åŸå› åˆ†ç±»
            zero_recall = [r for r in failed_cases if r["recall"] == 0]
            partial_recall = [r for r in failed_cases if 0 < r["recall"] < 1]

            logger.info(f"   å®Œå…¨æœªå‘½ä¸­ (å¬å›ç‡=0): {len(zero_recall)}ä¸ª")
            logger.info(f"   éƒ¨åˆ†å‘½ä¸­ (0<å¬å›ç‡<1): {len(partial_recall)}ä¸ª")

            # æ˜¾ç¤ºå‡ ä¸ªå…¸å‹å¤±è´¥æ¡ˆä¾‹
            logger.info(f"   å…¸å‹å¤±è´¥æ¡ˆä¾‹:")
            for i, r in enumerate(failed_cases[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                logger.info(f"     {i + 1}. {r['pdf_path']}: å¬å›ç‡={r['recall']:.3f}, F1={r['f1']:.3f}")

        # 5. æœ€ä½³è¡¨ç°æ¡ˆä¾‹
        best_cases = sorted(results, key=lambda x: x["f1"], reverse=True)[:5]
        logger.info(f"\nâœ… æœ€ä½³è¡¨ç°æ¡ˆä¾‹ (Top 5):")
        for i, r in enumerate(best_cases):
            logger.info(
                f"   {i + 1}. {r['pdf_path']}: F1={r['f1']:.4f}, å¬å›ç‡={r['recall']:.4f}, ç²¾ç¡®ç‡={r['precision']:.4f}")

        # 6. ä¿å­˜æ±‡æ€»æŠ¥å‘Šåˆ°æ–‡ä»¶
        summary_file = os.path.join(self.config['results_dir'], 'text_only_baseline_summary_50.json')
        summary_data = {
            "overview": {
                "total_samples": len(results),
                "success_count": sum(1 for r in results if r["success"]),
                "success_rate": sum(1 for r in results if r["success"]) / len(results) * 100,
                "avg_recall": float(np.mean([r["recall"] for r in results])),
                "avg_precision": float(np.mean([r["precision"] for r in results])),
                "avg_f1": float(np.mean([r["f1"] for r in results])),
                "avg_time": float(np.mean([r["retrieval_time"] for r in results]))
            },
            "task_analysis": {
                task: {
                    "count": stats["count"],
                    "success_rate": stats["success"] / stats["count"] * 100,
                    "avg_f1": float(np.mean(stats["f1s"])),
                    "avg_time": float(np.mean(stats["times"]))
                }
                for task, stats in task_stats.items()
            },
            "page_analysis": {
                str(num_pages): {
                    "count": stats["count"],
                    "success_rate": stats["success"] / stats["count"] * 100,
                    "avg_f1": float(np.mean(stats["f1s"]))
                }
                for num_pages, stats in page_stats.items()
            },
            "performance_distribution": {
                range_name: len(range_results)
                for range_name, range_results in f1_ranges.items()
            }
        }

        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ“„ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {summary_file}")
        logger.info(f"{'ğŸ”¥' * 60}")

    def run(self):
        """è¿è¡Œæµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹çº¯æ–‡æœ¬å¤šæ„å›¾æ£€ç´¢æµ‹è¯•ï¼ˆ50æ¡æ•°æ®ï¼‰...")
        start_time = time.time()

        try:
            results = self.test_text_only_retrieval()
            total_time = time.time() - start_time

            logger.info(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
            logger.info(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time / 60:.1f}åˆ†é’Ÿ)")
            logger.info(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {self.config['results_dir']}")

            # ğŸ”¥ æœ€ç»ˆæ±‡æ€»
            if results:
                success_rate = sum(1 for r in results if r["success"]) / len(results) * 100
                avg_f1 = np.mean([r["f1"] for r in results])
                logger.info(f"\nğŸ¯ æœ€ç»ˆæ±‡æ€»:")
                logger.info(f"   æµ‹è¯•æ ·æœ¬: {len(results)}")
                logger.info(f"   æˆåŠŸç‡: {success_rate:.1f}%")
                logger.info(f"   å¹³å‡F1: {avg_f1:.4f}")

        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}", exc_info=True)


class TextOnlyMatcher:
    """çº¯æ–‡æœ¬åŒ¹é…å™¨"""

    def __init__(self, bge_model_path: str, device: str = "cuda:0", topk: int = 10):
        self.bge_model_path = bge_model_path
        self.device = device
        self.topk = topk
        self._setup_models()

    def _setup_models(self):
        """è®¾ç½®æ–‡æœ¬æ¨¡å‹"""
        from transformers import AutoTokenizer, AutoModel

        logger.info("ğŸ”§ åˆå§‹åŒ–çº¯æ–‡æœ¬æ¨¡å‹...")
        self.text_tokenizer = AutoTokenizer.from_pretrained(
            self.bge_model_path,
            use_fast=True,
            local_files_only=True
        )
        self.text_model = AutoModel.from_pretrained(
            self.bge_model_path,
            local_files_only=True
        ).to(self.device)
        logger.info("âœ… çº¯æ–‡æœ¬æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    def retrieve(self, query: str, documents: list) -> list:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if not documents:
            return []

        try:
            # è®¡ç®—æŸ¥è¯¢åµŒå…¥
            query_embedding = self._compute_text_embedding(query)

            # è®¡ç®—æ–‡æ¡£åµŒå…¥å’Œç›¸ä¼¼åº¦
            scored_documents = []
            for doc in documents:
                text = doc.get("text", "")
                if not text.strip():
                    continue

                doc_embedding = self._compute_text_embedding(text)
                similarity = self._compute_similarity(query_embedding, doc_embedding)

                scored_documents.append({
                    "text": text,
                    "score": float(similarity),
                    "metadata": doc.get("metadata", {})
                })

            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            scored_documents.sort(key=lambda x: x["score"], reverse=True)
            return scored_documents[:self.topk]

        except Exception as e:
            logger.error(f"âŒ æ£€ç´¢å¤±è´¥: {str(e)}")
            return []

    def _compute_text_embedding(self, text: str):
        """è®¡ç®—æ–‡æœ¬åµŒå…¥"""
        inputs = self.text_tokenizer(
            text,
            padding=True,
            truncation=True,
            return_tensors="pt",
            max_length=512
        ).to(self.device)

        with torch.no_grad():
            outputs = self.text_model(**inputs)
            # ä½¿ç”¨[CLS]å‘é‡
            embedding = outputs.last_hidden_state[:, 0].cpu().numpy()

        return embedding

    def _compute_similarity(self, query_emb, doc_emb):
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        # å½’ä¸€åŒ–
        query_norm = query_emb / (np.linalg.norm(query_emb, axis=1, keepdims=True) + 1e-8)
        doc_norm = doc_emb / (np.linalg.norm(doc_emb, axis=1, keepdims=True) + 1e-8)

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = np.dot(query_norm, doc_norm.T)
        return similarity[0][0]


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ çº¯æ–‡æœ¬Baselineå¤šæ„å›¾æ£€ç´¢æµ‹è¯•ï¼ˆ50æ¡æ•°æ®ï¼‰")
    print("=" * 60)
    print("ğŸ“„ ä½¿ç”¨å¤šæ„å›¾æ‹†è§£ + çº¯æ–‡æœ¬æ£€ç´¢ï¼ˆè·³è¿‡å›¾åƒæ¨¡å‹ï¼‰")
    print("ğŸ”¥ æµ‹è¯•50æ¡æ•°æ®å¹¶ç”Ÿæˆè¯¦ç»†æ±‡æ€»æŠ¥å‘Š")
    print("=" * 60)

    parser = argparse.ArgumentParser(description="çº¯æ–‡æœ¬å¤šæ„å›¾æ£€ç´¢æµ‹è¯•å·¥å…·")
    parser.add_argument("--sample_size", type=int, default=50, help="æµ‹è¯•æ ·æœ¬æ•°é‡")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼")

    try:
        args = parser.parse_args()
    except SystemExit:
        args = argparse.Namespace(sample_size=50, debug=False)

    logger.info(f"ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°é‡: {args.sample_size}")
    logger.info(f"ğŸ› è°ƒè¯•æ¨¡å¼: {args.debug}")

    # å…ˆæ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    bge_path = "/root/autodl-tmp/multimodal-RAG/hf_models/bge-large-en-v1.5"
    reranker_path = "/root/autodl-tmp/multimodal-RAG/hf_models/bge-reranker-large"

    if not os.path.exists(bge_path):
        print(f"âŒ BGEæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {bge_path}")
        return

    if not os.path.exists(reranker_path):
        print(f"âŒ é‡æ’åºå™¨è·¯å¾„ä¸å­˜åœ¨: {reranker_path}")
        return

    tester = TextOnlyMultiIntentTester()
    if args.sample_size:
        tester.config['sample_size'] = args.sample_size

    tester.run()


if __name__ == "__main__":
    main()