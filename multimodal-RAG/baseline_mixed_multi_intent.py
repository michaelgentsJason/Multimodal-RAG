#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import json
import time
import argparse
from pathlib import Path
from tqdm import tqdm
import numpy as np
from dotenv import load_dotenv
import torch
from pdf2image import convert_from_path
from PIL import Image
import logging
from DeepRAG_Multimodal.deep_retrieve.retriever_multimodal_bge import MultimodalMatcher, RetrieverConfig

# åˆ›å»ºæ—¥å¿—
log_dir = Path("./log")
log_dir.mkdir(exist_ok=True)
log_file = log_dir / "text_only_baseline_test.log"

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(str(log_file), mode='a', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

logger.info("=== çº¯æ–‡æœ¬Baselineå¤šæ„å›¾æ£€ç´¢æµ‹è¯•å¼€å§‹ ===")

# æ·»åŠ è·¯å¾„
sys.path.append("multimodal-RAG/DeepRAG_Multimodal/deep_retrieve")

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv("/root/autodl-tmp/multimodal-RAG/multimodal-RAG/DeepRAG_Multimodal/configs/.env")

# å¯¼å…¥å¿…è¦çš„åº“
from DeepRAG_Multimodal.deep_retrieve.ming.deepsearch_optimize_ming import DeepSearch_Beta


class TextOnlyMultiIntentTester:
    """æ··åˆå¤šæ„å›¾æ£€ç´¢æµ‹è¯•ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.config = self.load_config()
        os.makedirs(self.config['results_dir'], exist_ok=True)
        self.verify_model_files()
        self.setup_models()

    def load_config(self):
        """åŠ è½½é…ç½®"""
        config = {
            # è·¯å¾„é…ç½®
            'test_data_path': '/root/autodl-tmp/multimodal-RAG/multimodal-RAG/DeepRAG_Multimodal/picked_LongDoc/selected_LongDocURL_public_with_subtask_category.jsonl',
            'pdf_base_dir': '/root/autodl-tmp/multimodal-RAG/multimodal-RAG/DeepRAG_Multimodal/picked_LongDoc',
            'results_dir': './test_results',

            # é‡‡æ ·é…ç½®
            'sample_size': 1,  # é»˜è®¤50æ¡æ•°æ®
            'debug': True,

            # æ£€ç´¢é…ç½®
            'max_iterations': 2,
            'embedding_topk': 12,
            'rerank_topk': 5,
            'text_weight': 0.7,
            'image_weight': 0.3,

            # æ¨¡å‹é…ç½® - åªéœ€è¦æ–‡æœ¬æ¨¡å‹
            'mm_model_name': "/root/autodl-tmp/multimodal-RAG/hf_models/colqwen2.5-v0.2",
            'mm_processor_name': "/root/autodl-tmp/multimodal-RAG/hf_models/colqwen2.5-v0.1",
            'bge_model_name': "/root/autodl-tmp/multimodal-RAG/hf_models/bge-large-en-v1.5",
            'reranker_model_name': "/root/autodl-tmp/multimodal-RAG/hf_models/bge-reranker-large",

            'retrieval_mode': 'mixed',  # 'mixed', 'text_only', 'image_only'

            'device': 'cuda:0',
            'batch_size': 4,
            'ocr_method': 'pytesseract',
        }
        return config

    def verify_model_files(self):
        """éªŒè¯æ¨¡å‹æ–‡ä»¶"""
        model_paths = [
            self.config['bge_model_name'],
            self.config['reranker_model_name']
        ]

        for model_path in model_paths:
            if not os.path.exists(model_path):
                logger.error(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
                raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            else:
                logger.info(f"âœ… æ¨¡å‹è·¯å¾„éªŒè¯æˆåŠŸ: {model_path}")

    def setup_models(self):
        """åˆå§‹åŒ–æ£€ç´¢æ¨¡å‹"""
        logger.info("ğŸš€ åˆå§‹åŒ–çº¯æ–‡æœ¬å¤šæ„å›¾æ£€ç´¢æ¨¡å‹...")
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        logger.info(f"ğŸ® ä½¿ç”¨è®¾å¤‡: {device}")

        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated() / (1024 ** 3)
            logger.info(f"ğŸ§¹ åˆå§‹GPUå†…å­˜ä½¿ç”¨: {initial_memory:.2f}GB")

        try:
            # åˆå§‹åŒ–é‡æ’åºå™¨
            from FlagEmbedding import FlagReranker

            logger.info("â³ åˆå§‹åŒ–é‡æ’åºå™¨...")
            self.reranker = FlagReranker(
                model_name_or_path=self.config['reranker_model_name'],
                use_fp16=True,
                device=device,
                local_files_only=True
            )
            logger.info("âœ… é‡æ’åºå™¨åˆå§‹åŒ–æˆåŠŸ")

            # åˆå§‹åŒ–çº¯æ–‡æœ¬åŒ¹é…å™¨
            # logger.info("â³ åˆå§‹åŒ–çº¯æ–‡æœ¬åŒ¹é…å™¨...")
            # self.text_matcher = TextOnlyMatcher(
            #     bge_model_path=self.config['bge_model_name'],
            #     device=device,
            #     topk=self.config['rerank_topk']
            # )
            # logger.info("âœ… çº¯æ–‡æœ¬åŒ¹é…å™¨åˆå§‹åŒ–æˆåŠŸ")

            logger.info("â³ åˆå§‹åŒ–å¤šæ¨¡æ€åŒ¹é…å™¨...")
            retriever_config = RetrieverConfig(
                model_name=self.config['mm_model_name'],
                processor_name=self.config['mm_processor_name'],
                bge_model_name=self.config['bge_model_name'],
                device=self.config['device'],
                use_fp16=True,
                batch_size=self.config['batch_size'],
                mode=self.config['retrieval_mode'],  # 'mixed'
                ocr_method=self.config['ocr_method']
            )

            self.text_matcher = MultimodalMatcher(
                config=retriever_config,
                embedding_weight=self.config['text_weight'],
                topk=self.config['rerank_topk']
            )
            logger.info("âœ… å¤šæ¨¡æ€åŒ¹é…å™¨åˆå§‹åŒ–æˆåŠŸ")

            # åˆå§‹åŒ–å•æ„å›¾æ£€ç´¢å™¨
            logger.info("â³ åˆå§‹åŒ–å•æ„å›¾æ£€ç´¢å™¨...")
            self.single_intent_search = DeepSearch_Beta(
                max_iterations=self.config['max_iterations'],
                reranker=self.reranker,
                params={
                    "embedding_topk": self.config['embedding_topk'],
                    "rerank_topk": self.config['rerank_topk'],
                    "text_weight": self.config['text_weight'],
                    "image_weight": self.config['image_weight']
                }
            )
            # # é‡å†™æ–¹æ³•ï¼Œå¼ºåˆ¶å•æ„å›¾
            # self.single_intent_search._split_query_intent = lambda query: [query]
            # self.single_intent_search._refine_query_intent = lambda original_query, intent_queries, context: [
            #     original_query]

            # åˆå§‹åŒ–å¤šæ„å›¾æ£€ç´¢å™¨
            logger.info("â³ åˆå§‹åŒ–å¤šæ„å›¾æ£€ç´¢å™¨...")
            self.multi_intent_search = DeepSearch_Beta(
                max_iterations=self.config['max_iterations'],
                reranker=self.reranker,
                params={
                    "embedding_topk": self.config['embedding_topk'],
                    "rerank_topk": self.config['rerank_topk'],
                    "text_weight": self.config['text_weight'],
                    "image_weight": self.config['image_weight']
                }
            )
            logger.info("âœ… å¤šæ„å›¾æ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸ")

            if torch.cuda.is_available():
                final_memory = torch.cuda.memory_allocated() / (1024 ** 3)
                logger.info(f"ğŸ“Š æœ€ç»ˆGPUå†…å­˜ä½¿ç”¨: {final_memory:.2f}GB")

            logger.info("âœ… çº¯æ–‡æœ¬æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            raise

    def load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        allowed_doc_nos = [
            '4046173.pdf', '4176503.pdf', '4057524.pdf', '4064501.pdf', '4057121.pdf', '4174854.pdf',
            '4148165.pdf', '4129570.pdf', '4010333.pdf', '4147727.pdf', '4066338.pdf', '4031704.pdf',
            '4050613.pdf', '4072260.pdf', '4091919.pdf', '4094684.pdf', '4063393.pdf', '4132494.pdf',
            '4185438.pdf', '4129670.pdf', '4138347.pdf', '4190947.pdf', '4100212.pdf', '4173940.pdf',
            '4069930.pdf', '4174181.pdf', '4027862.pdf', '4012567.pdf', '4145761.pdf', '4078345.pdf',
            '4061601.pdf', '4170122.pdf', '4077673.pdf', '4107960.pdf', '4005877.pdf', '4196005.pdf',
            '4126467.pdf', '4088173.pdf', '4106951.pdf', '4086173.pdf', '4072232.pdf', '4111230.pdf',
            '4057714.pdf'
        ]

        logger.info(f"ğŸ“š åŠ è½½æµ‹è¯•æ•°æ®: {self.config['test_data_path']}")
        test_data = []

        with open(self.config['test_data_path'], 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    item = json.loads(line)
                    test_data.append(item)

        if self.config['sample_size'] > 0 and len(test_data) > self.config['sample_size']:
            np.random.seed(42)
            test_data = np.random.choice(test_data, self.config['sample_size'], replace=False).tolist()

        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(test_data)} æ¡æµ‹è¯•æ•°æ®")
        return test_data

    def process_single_document(self, doc_data):
        """å¤„ç†å•ä¸ªæ–‡æ¡£ - ä»…æå–æ–‡æœ¬"""
        documents = []
        pdf_path = os.path.join(self.config['pdf_base_dir'], doc_data["pdf_path"])

        # è·å–OCRç»“æœ
        ocr_file = os.path.join(
            self.config['pdf_base_dir'],
            f"{self.config['ocr_method']}_save",
            f"{os.path.basename(doc_data['pdf_path']).replace('.pdf', '.json')}"
        )

        if os.path.exists(ocr_file):
            with open(ocr_file, 'r', encoding='utf-8') as f:
                loaded_data = json.load(f)
            logger.info(f"ğŸ“– æˆåŠŸè¯»å–OCRæ–‡ä»¶: {ocr_file}")
        else:
            logger.warning(f"âš ï¸ æ‰¾ä¸åˆ°OCRæ–‡ä»¶: {ocr_file}")
            return []

        # å›¾åƒï¼Œæ–‡æœ¬
        page_keys = list(loaded_data.keys())
        for idx, page_key in enumerate(page_keys):
            page_text = loaded_data[page_key]
            if not page_text.strip():
                page_text = f"ç¬¬{idx + 1}é¡µå†…å®¹"

            page_image = None
            if self.config['retrieval_mode'] == 'mixed':
                try:
                    # è½¬æ¢PDFé¡µé¢ä¸ºå›¾åƒ
                    pdf_path = os.path.join(self.config['pdf_base_dir'], doc_data["pdf_path"])
                    pages = convert_from_path(pdf_path)
                    if idx < len(pages):
                        page_image = pages[idx]
                except Exception as e:
                    logger.warning(f"âš ï¸ æ— æ³•åŠ è½½å›¾åƒé¡µé¢ {idx + 1}: {str(e)}")

            documents.append({
                "text": page_text,
                "image": page_image,  # æ·»åŠ å›¾åƒ
                "metadata": {
                    "page_index": idx + 1,
                    "pdf_path": doc_data.get("pdf_path", "")
                }
            })

        logger.info(f"ğŸ“‘ æˆåŠŸåˆ›å»º {len(documents)} ä¸ªæ–‡æ¡£å¯¹è±¡ï¼ˆæ··åˆæ¨¡å¼ï¼‰")

        # æ–‡æœ¬è´¨é‡æ£€æŸ¥
        total_text_length = sum(len(doc['text']) for doc in documents)
        logger.info(f"ğŸ“ æ€»æ–‡æœ¬é•¿åº¦: {total_text_length} å­—ç¬¦")

        return documents

    def evaluate_results(self, results, evidence_pages, method_name):
        """è¯„ä¼°æ£€ç´¢ç»“æœ"""
        # ä¿æŒé¡µé¢å’Œåˆ†æ•°çš„å¯¹åº”å…³ç³»
        page_score_pairs = []
        for result in results:
            page = None
            if 'metadata' in result and 'page_index' in result['metadata']:
                page = result['metadata']['page_index']
            elif 'page' in result and result['page'] is not None:
                page = result['page']

            if page is not None:
                page_score_pairs.append((page, result.get('score', 0)))

        # æŒ‰åˆ†æ•°æ’åºï¼ˆä¿æŒå¯¹åº”å…³ç³»ï¼‰
        page_score_pairs.sort(key=lambda x: x[1], reverse=True)

        retrieved_pages = [pair[0] for pair in page_score_pairs]
        retrieval_scores = [pair[1] for pair in page_score_pairs]

        evidence_set = set(evidence_pages)
        correct_pages = evidence_set.intersection(set(retrieved_pages))

        recall = len(correct_pages) / len(evidence_set) if evidence_set else 0
        precision = len(correct_pages) / len(retrieved_pages) if retrieved_pages else 0
        f1 = 2 * recall * precision / (recall + precision) if (recall + precision) > 0 else 0

        logger.info(f"ğŸ“Š {method_name}ç»“æœ:")
        logger.info(f"   ğŸ¯ æ£€ç´¢åˆ°é¡µé¢: {retrieved_pages[:5]}")  # æŒ‰åˆ†æ•°é¡ºåºæ˜¾ç¤º
        logger.info(f"   âœ… æ­£ç¡®é¡µé¢: {sorted(list(correct_pages))}")
        logger.info(f"   ğŸ“Š æ£€ç´¢åˆ†æ•°: {retrieval_scores[:5]}")
        logger.info(f"   ğŸ“Š é¡µé¢-åˆ†æ•°å¯¹åº”: {list(zip(retrieved_pages[:5], retrieval_scores[:5]))}")

        return {
            "retrieved_pages": retrieved_pages,
            "correct_pages": list(correct_pages),
            "recall": recall,
            "precision": precision,
            "f1": f1,
            "retrieval_scores": retrieval_scores[:10],
            "success": len(correct_pages) == len(evidence_set)
        }

    def test_text_only_retrieval(self):
        """æµ‹è¯•çº¯æ–‡æœ¬å•æ„å›¾ vs å¤šæ„å›¾æ£€ç´¢å¯¹æ¯”"""
        logger.info("ğŸ¯ å¼€å§‹çº¯æ–‡æœ¬å•æ„å›¾ vs å¤šæ„å›¾æ£€ç´¢å¯¹æ¯”æµ‹è¯•...")
        test_data = self.load_test_data()
        results = []

        for idx, doc_data in enumerate(tqdm(test_data, desc="å•æ„å›¾vså¤šæ„å›¾å¯¹æ¯”æµ‹è¯•")):
            try:
                query = doc_data.get("question", "")
                evidence_pages = doc_data.get("evidence_pages", [])

                logger.info(f"\n{'=' * 60}")
                logger.info(f"ğŸ” å¤„ç†æ–‡æ¡£ {idx + 1}/{len(test_data)}: {doc_data.get('pdf_path', 'Unknown')}")
                logger.info(f"â“ æŸ¥è¯¢: {query}")
                logger.info(f"ğŸ“‹ è¯æ®é¡µé¢: {evidence_pages}")

                document_pages = self.process_single_document(doc_data)
                if not document_pages:
                    logger.warning(f"âš ï¸ è·³è¿‡æ–‡æ¡£: æ— æœ‰æ•ˆå†…å®¹")
                    continue

                data = {"query": query, "documents": document_pages}

                # å•æ„å›¾æ£€ç´¢
                logger.info("ğŸ“„ å¼€å§‹å•æ„å›¾æ£€ç´¢...")
                single_start_time = time.time()
                single_results = self.single_intent_search.search_retrieval(data, multi_intent=False, retriever=self.text_matcher)
                single_elapsed = time.time() - single_start_time

                # å¤šæ„å›¾æ£€ç´¢
                logger.info("ğŸ“„ å¼€å§‹å¤šæ„å›¾æ£€ç´¢...")
                multi_start_time = time.time()
                multi_results = self.multi_intent_search.search_retrieval(data, multi_intent=True, retriever=self.text_matcher)
                multi_elapsed = time.time() - multi_start_time

                # è¯„ä¼°å•æ„å›¾ç»“æœ
                single_eval = self.evaluate_results(single_results, evidence_pages, "å•æ„å›¾")

                # è¯„ä¼°å¤šæ„å›¾ç»“æœ
                multi_eval = self.evaluate_results(multi_results, evidence_pages, "å¤šæ„å›¾")

                # logger.info(f"â±ï¸ å•æ„å›¾æ£€ç´¢è€—æ—¶: {single_elapsed:.2f}ç§’")
                logger.info(f"â±ï¸ å¤šæ„å›¾æ£€ç´¢è€—æ—¶: {multi_elapsed:.2f}ç§’")

                # è®°å½•å¯¹æ¯”ç»“æœ
                result = {
                    "doc_id": doc_data.get("doc_no", ""),
                    "pdf_path": doc_data.get("pdf_path", ""),
                    "query": query,
                    "evidence_pages": evidence_pages,
                    "task_tag": doc_data.get("task_tag", ""),
                    "subTask": doc_data.get("subTask", []),

                    # å•æ„å›¾ç»“æœ
                    "single_intent": {
                        **single_eval,
                        "retrieval_time": single_elapsed
                    },

                    # å¤šæ„å›¾ç»“æœ
                    "multi_intent": {
                        **multi_eval,
                        "retrieval_time": multi_elapsed
                    },

                    # å¯¹æ¯”æŒ‡æ ‡
                    "comparison": {
                        "f1_improvement": multi_eval["f1"] - single_eval["f1"],
                        "recall_improvement": multi_eval["recall"] - single_eval["recall"],
                        "precision_improvement": multi_eval["precision"] - single_eval["precision"],
                        "time_overhead": multi_elapsed - single_elapsed,
                        "multi_intent_better": multi_eval["f1"] > single_eval["f1"]
                    }
                }

                results.append(result)

            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
                import traceback
                traceback.print_exc()

        #ä¿å­˜å’Œåˆ†æç»“æœ
        result_file = os.path.join(self.config['results_dir'], 'single_vs_multi_intent_comparison.json')
        result_file = os.path.join(self.config['results_dir'], 'improved_multi_intent_comparison.json')
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        self.analyze_comparison_results(results)
        # self.generate_comparison_summary(results)
        logger.info(f"ğŸ‰ å¯¹æ¯”æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
        return results

    def analyze_comparison_results(self, results):
        """åˆ†æå¯¹æ¯”æµ‹è¯•ç»“æœ"""
        if not results:
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„ç»“æœè¿›è¡Œåˆ†æ")
            return

        # å•æ„å›¾æŒ‡æ ‡
        single_recalls = [r["single_intent"]["recall"] for r in results]
        single_precisions = [r["single_intent"]["precision"] for r in results]
        single_f1s = [r["single_intent"]["f1"] for r in results]
        single_times = [r["single_intent"]["retrieval_time"] for r in results]
        single_success_count = sum(1 for r in results if r["single_intent"]["success"])

        # å¤šæ„å›¾æŒ‡æ ‡
        multi_recalls = [r["multi_intent"]["recall"] for r in results]
        multi_precisions = [r["multi_intent"]["precision"] for r in results]
        multi_f1s = [r["multi_intent"]["f1"] for r in results]
        multi_times = [r["multi_intent"]["retrieval_time"] for r in results]
        multi_success_count = sum(1 for r in results if r["multi_intent"]["success"])

        # æ”¹è¿›æŒ‡æ ‡
        f1_improvements = [r["comparison"]["f1_improvement"] for r in results]
        multi_better_count = sum(1 for r in results if r["comparison"]["multi_intent_better"])

        logger.info(f"\n{'=' * 80}")
        # logger.info(f"ğŸ“Š å•æ„å›¾ vs å¤šæ„å›¾æ£€ç´¢æ€§èƒ½å¯¹æ¯”åˆ†æ")
        logger.info(f"æ”¹è¿›å¤šæ„å›¾æ£€ç´¢æ€§èƒ½å¯¹æ¯”åˆ†æ")
        logger.info(f"{'=' * 80}")
        logger.info(f"ğŸ“‹ æµ‹è¯•æ–‡æ¡£æ•°: {len(results)}")

        logger.info(f"\nğŸ”¹ å•æ„å›¾æ£€ç´¢æ€§èƒ½:")
        logger.info(f"   å¹³å‡å¬å›ç‡: {np.mean(single_recalls):.4f}")
        logger.info(f"   å¹³å‡ç²¾ç¡®ç‡: {np.mean(single_precisions):.4f}")
        logger.info(f"   å¹³å‡F1å€¼: {np.mean(single_f1s):.4f}")
        logger.info(f"   å¹³å‡æ£€ç´¢æ—¶é—´: {np.mean(single_times):.2f}ç§’")
        logger.info(
            f"   æˆåŠŸç‡: {(single_success_count / len(results)) * 100:.2f}% ({single_success_count}/{len(results)})")

        logger.info(f"\nğŸ”¹ å¤šæ„å›¾æ£€ç´¢æ€§èƒ½:")
        logger.info(f"   å¹³å‡å¬å›ç‡: {np.mean(multi_recalls):.4f}")
        logger.info(f"   å¹³å‡ç²¾ç¡®ç‡: {np.mean(multi_precisions):.4f}")
        logger.info(f"   å¹³å‡F1å€¼: {np.mean(multi_f1s):.4f}")
        logger.info(f"   å¹³å‡æ£€ç´¢æ—¶é—´: {np.mean(multi_times):.2f}ç§’")
        logger.info(
            f"   æˆåŠŸç‡: {(multi_success_count / len(results)) * 100:.2f}% ({multi_success_count}/{len(results)})")

        logger.info(f"\nğŸ”¸ æ€§èƒ½æå‡åˆ†æ:")
        logger.info(f"   å¹³å‡F1æå‡: {np.mean(f1_improvements):+.4f}")
        logger.info(f"   å¹³å‡å¬å›ç‡æå‡: {np.mean([r['comparison']['recall_improvement'] for r in results]):+.4f}")
        logger.info(f"   å¹³å‡ç²¾ç¡®ç‡æå‡: {np.mean([r['comparison']['precision_improvement'] for r in results]):+.4f}")
        logger.info(f"   å¹³å‡æ—¶é—´å¼€é”€: {np.mean([r['comparison']['time_overhead'] for r in results]):+.2f}ç§’")
        logger.info(
            f"   å¤šæ„å›¾ä¼˜äºå•æ„å›¾çš„æ¯”ä¾‹: {(multi_better_count / len(results)) * 100:.2f}% ({multi_better_count}/{len(results)})")

        logger.info(f"{'=' * 80}")

    def generate_comparison_summary(self, results):
        """ç”Ÿæˆå¯¹æ¯”æ±‡æ€»æŠ¥å‘Š"""
        if not results:
            return

        logger.info(f"\n{'=' * 80}")
        logger.info(f"ğŸ“‹ å•æ„å›¾ vs å¤šæ„å›¾æ£€ç´¢ - æ€»ä½“å¯¹æ¯”æ±‡æ€»")
        logger.info(f"{'=' * 80}")

        total_docs = len(results)

        # åŸºç¡€ç»Ÿè®¡
        single_success = sum(1 for r in results if r["single_intent"]["success"])
        multi_success = sum(1 for r in results if r["multi_intent"]["success"])
        multi_better_count = sum(1 for r in results if r["comparison"]["multi_intent_better"])

        # æ€§èƒ½æŒ‡æ ‡
        single_avg_f1 = np.mean([r["single_intent"]["f1"] for r in results])
        multi_avg_f1 = np.mean([r["multi_intent"]["f1"] for r in results])
        avg_f1_improvement = np.mean([r["comparison"]["f1_improvement"] for r in results])

        single_total_time = sum([r["single_intent"]["retrieval_time"] for r in results])
        multi_total_time = sum([r["multi_intent"]["retrieval_time"] for r in results])

        # æŒ‰ä»»åŠ¡ç±»å‹åˆ†æ
        task_stats = {}
        for r in results:
            task_tag = r.get("task_tag", "Unknown")
            if task_tag not in task_stats:
                task_stats[task_tag] = {
                    "count": 0,
                    "single_success": 0,
                    "multi_success": 0,
                    "single_f1_sum": 0,
                    "multi_f1_sum": 0,
                    "multi_better": 0
                }
            task_stats[task_tag]["count"] += 1
            if r["single_intent"]["success"]:
                task_stats[task_tag]["single_success"] += 1
            if r["multi_intent"]["success"]:
                task_stats[task_tag]["multi_success"] += 1
            task_stats[task_tag]["single_f1_sum"] += r["single_intent"]["f1"]
            task_stats[task_tag]["multi_f1_sum"] += r["multi_intent"]["f1"]
            if r["comparison"]["multi_intent_better"]:
                task_stats[task_tag]["multi_better"] += 1

        logger.info(f"ğŸ¯ æ€»ä½“å¯¹æ¯”ç»“æœ:")
        logger.info(f"   æµ‹è¯•æ–‡æ¡£æ€»æ•°: {total_docs}")
        logger.info(f"   å•æ„å›¾æˆåŠŸæ•°: {single_success} ({(single_success / total_docs) * 100:.2f}%)")
        logger.info(f"   å¤šæ„å›¾æˆåŠŸæ•°: {multi_success} ({(multi_success / total_docs) * 100:.2f}%)")
        logger.info(f"   å¤šæ„å›¾ä¼˜äºå•æ„å›¾: {multi_better_count} ({(multi_better_count / total_docs) * 100:.2f}%)")
        logger.info(f"   å•æ„å›¾å¹³å‡F1: {single_avg_f1:.4f}")
        logger.info(f"   å¤šæ„å›¾å¹³å‡F1: {multi_avg_f1:.4f}")
        logger.info(f"   å¹³å‡F1æå‡: {avg_f1_improvement:+.4f}")
        logger.info(f"   å•æ„å›¾æ€»è€—æ—¶: {single_total_time:.2f}ç§’")
        logger.info(f"   å¤šæ„å›¾æ€»è€—æ—¶: {multi_total_time:.2f}ç§’")
        logger.info(f"   æ—¶é—´å¼€é”€: {multi_total_time - single_total_time:+.2f}ç§’")

        logger.info(f"\nğŸ“Š æŒ‰ä»»åŠ¡ç±»å‹å¯¹æ¯”:")
        for task_tag, stats in task_stats.items():
            count = stats["count"]
            single_success_rate = (stats["single_success"] / count) * 100
            multi_success_rate = (stats["multi_success"] / count) * 100
            single_avg_f1 = stats["single_f1_sum"] / count
            multi_avg_f1 = stats["multi_f1_sum"] / count
            multi_better_rate = (stats["multi_better"] / count) * 100

            logger.info(f"   {task_tag} ({count}æ ·æœ¬):")
            logger.info(f"     å•æ„å›¾: æˆåŠŸç‡{single_success_rate:.1f}%, F1:{single_avg_f1:.4f}")
            logger.info(f"     å¤šæ„å›¾: æˆåŠŸç‡{multi_success_rate:.1f}%, F1:{multi_avg_f1:.4f}")
            logger.info(f"     å¤šæ„å›¾ä¼˜åŠ¿: {multi_better_rate:.1f}%")

        # ä¿å­˜æ±‡æ€»åˆ°æ–‡ä»¶
        summary = {
            "experiment_name": "å•æ„å›¾vså¤šæ„å›¾æ£€ç´¢å¯¹æ¯”",
            "total_documents": total_docs,
            "single_intent_results": {
                "successful_retrievals": single_success,
                "success_rate": (single_success / total_docs) * 100,
                "average_f1": single_avg_f1,
                "total_time": single_total_time
            },
            "multi_intent_results": {
                "successful_retrievals": multi_success,
                "success_rate": (multi_success / total_docs) * 100,
                "average_f1": multi_avg_f1,
                "total_time": multi_total_time
            },
            "comparison_metrics": {
                "multi_intent_better_count": multi_better_count,
                "multi_intent_better_rate": (multi_better_count / total_docs) * 100,
                "average_f1_improvement": avg_f1_improvement,
                "time_overhead": multi_total_time - single_total_time
            },
            "task_breakdown": task_stats,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }

        summary_file = os.path.join(self.config['results_dir'], 'single_vs_multi_intent_summary.json')
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        logger.info(f"\nğŸ’¾ å¯¹æ¯”æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {summary_file}")
        logger.info(f"{'=' * 80}")

    def run(self):
        """è¿è¡Œæµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹å•æ„å›¾ vs å¤šæ„å›¾æ£€ç´¢å¯¹æ¯”æµ‹è¯•...")
        start_time = time.time()

        try:
            results = self.test_text_only_retrieval()
            total_time = time.time() - start_time
            logger.info(f"\nğŸ‰ å¯¹æ¯”æµ‹è¯•å®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}ç§’")
            logger.info(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {self.config['results_dir']}")

        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}", exc_info=True)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å•æ„å›¾ vs å¤šæ„å›¾æ£€ç´¢å¯¹æ¯”æµ‹è¯•")
    print("=" * 50)
    print("ğŸ“„ å¯¹æ¯”å•æ„å›¾å’Œå¤šæ„å›¾æ‹†è§£åœ¨çº¯æ–‡æœ¬æ£€ç´¢ä¸­çš„æ•ˆæœå·®å¼‚")
    print("=" * 50)

    parser = argparse.ArgumentParser(description="å•æ„å›¾vså¤šæ„å›¾æ£€ç´¢å¯¹æ¯”æµ‹è¯•å·¥å…·")
    parser.add_argument("--sample_size", type=int, default=50, help="æµ‹è¯•æ ·æœ¬æ•°é‡")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼")

    try:
        args = parser.parse_args()
    except SystemExit:
        args = argparse.Namespace(sample_size=50, debug=False)

    logger.info(f"ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°é‡: {args.sample_size}")
    logger.info(f"ğŸ› è°ƒè¯•æ¨¡å¼: {args.debug}")

    # å…ˆæ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    bge_path = "/root/autodl-tmp/multimodal-RAG/hf_models/bge-large-en-v1.5"
    reranker_path = "/root/autodl-tmp/multimodal-RAG/hf_models/bge-reranker-large"

    if not os.path.exists(bge_path):
        print(f"âŒ BGEæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {bge_path}")
        return

    if not os.path.exists(reranker_path):
        print(f"âŒ é‡æ’åºå™¨è·¯å¾„ä¸å­˜åœ¨: {reranker_path}")
        return

    tester = TextOnlyMultiIntentTester()
    if args.sample_size:
        tester.config['sample_size'] = args.sample_size

    tester.run()


if __name__ == "__main__":
    main()