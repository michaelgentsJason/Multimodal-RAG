import json
from copy import deepcopy
from typing import List, Dict, Annotated, Optional, Any
import sys
sys.path.append("DeepRAG_Multimodal/deep_retrieve")
from DeepRAG_Multimodal.deep_retrieve.ming.agent_gpt4 import AzureGPT4Chat, create_response_format
from datetime import datetime
from DeepRAG_Multimodal.deep_retrieve.tool_retriever_embed import EmbeddingMatcher
import asyncio
import concurrent.futures
from textwrap import dedent
from langchain_core.documents import Document
from FlagEmbedding import FlagReranker


class DeepSearch_Alpha:
    def __init__(self, max_iterations: int = 2, reranker: FlagReranker = None, params: dict = None):
        self.max_iterations = max_iterations
        self.reranker = reranker
        self.params = params

    def result_processor(self, results):
        matched_docs = []
        for doc, score in results:
            matched_docs.append({
                'text': doc.page_content,
                'score': 1 - score,
                'image_score': doc.metadata.get('image_score', 0),  # æ·»åŠ å›¾åƒåˆ†æ•°
                'text_score': doc.metadata.get('text_score', 0)   # æ·»åŠ æ–‡æœ¬åˆ†æ•°
            })
        return matched_docs

    def llm_rerank(self, query, retrieval_list, reranker, topk=None):
        pairs = [[query, doc['text']] for doc in retrieval_list]
        rerank_scores = reranker.compute_score(pairs, normalize=True)
        output_list = []
        for score, doc in sorted(zip(rerank_scores, retrieval_list), key=lambda x: x[0], reverse=True):
            output_list.append({
                'text': doc['text'],
                'score': score,
                'image_score': doc['image_score'],
                'text_score': doc['text_score']
            })
        if topk is not None:
            output_list = output_list[:topk]
        return output_list    

    def rerank_index_processor(self, results):
        """é»˜è®¤çš„ç›¸ä¼¼æ€§æœç´¢ç»“æœå¤„ç†å™¨"""
        matched_docs = []
        for doc, score in results:
            # åˆ›å»ºæ–°å­—å…¸è€Œä¸æ˜¯ä¿®æ”¹Documentå¯¹è±¡
            matched_doc = {
                'text': doc.page_content,
                'score': 1-score,
                # å¤åˆ¶å…ƒæ•°æ®ï¼ˆå¦‚æœæœ‰éœ€è¦ï¼‰
                **doc.metadata
            }
            matched_docs.append(matched_doc)
        return matched_docs

    def dynamic_retrieve_judge(
        self, 
        relevance_scores, 
        outlier_multiplier=1.5, 
        top_mean_threshold=0.7, 
        steepness_threshold=0.15
    ):
        """
        åŠ¨æ€é˜ˆå€¼ç®—æ³•ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦æœ‰è¶³å¤Ÿçš„ä¿¡æ¯ã€‚
        
        å‚æ•°:
        - relevance_scores (List[float]): æ–‡æ¡£çš„ç›¸å…³æ€§å¾—åˆ†åˆ—è¡¨ã€‚
        - outlier_multiplier (float): ç¦»ç¾¤å€¼è§„åˆ™çš„ä¹˜æ•°ã€‚å¢å¤§æ­¤å€¼ä¼šä½¿å¾—ç¦»ç¾¤å€¼è§„åˆ™æ›´ä¸¥æ ¼ã€‚
        - top_mean_threshold (float): å¤´éƒ¨å‡å€¼è§„åˆ™çš„é˜ˆå€¼ã€‚å¢å¤§æ­¤å€¼ä¼šä½¿å¾—å¤´éƒ¨å‡å€¼è§„åˆ™æ›´ä¸¥æ ¼ã€‚
        - steepness_threshold (float): é™¡åº¦æ£€æµ‹çš„é˜ˆå€¼ã€‚å‡å°æ­¤å€¼ä¼šä½¿å¾—é™¡åº¦æ£€æµ‹æ›´ä¸¥æ ¼ã€‚
        
        è¿”å›:
        - sufficient (bool): æ˜¯å¦æœ‰è¶³å¤Ÿçš„ä¿¡æ¯ã€‚
        """
        # åŠ¨æ€é˜ˆå€¼ç®—æ³•
        scores = sorted(relevance_scores, reverse=True)
        n = len(scores)
        
        # åŸºç¡€ç»Ÿè®¡é‡
        mean = sum(scores) / n
        std_dev = (sum((x - mean)**2 for x in scores)/n)**0.5
        max_score = scores[0]
        q3 = scores[int(n*0.25)]  # å‰25%åˆ†ä½æ•°
        
        # åŠ¨æ€é˜ˆå€¼è§„åˆ™ï¼ˆå¯ç»„åˆè°ƒæ•´ï¼‰ï¼š
        threshold_rules = [
            max_score > (mean + outlier_multiplier * std_dev),  # ç¦»ç¾¤å€¼è§„åˆ™
            (sum(scores[:int(n*0.25)])/int(n*0.25)) > top_mean_threshold if n >=4 else False,  # å¤´éƒ¨å‡å€¼è§„åˆ™
            (scores[0] - scores[2]) < steepness_threshold if n >=3 else False  # é™¡åº¦æ£€æµ‹
        ]
        
        # ç»„åˆåˆ¤æ–­é€»è¾‘ï¼ˆæ»¡è¶³ä»»æ„ä¸¤ä¸ªæ¡ä»¶ï¼‰
        sufficient = sum(threshold_rules) >= 2
        
        # å¢å¼ºé«˜ç½®ä¿¡åº¦æƒ…å†µ
        if max_score > 0.9 and scores[0] - scores[1] > 0.2:
            sufficient = True
        
        # å¢åŠ å¯¹å›¾åƒå’Œæ–‡æœ¬åˆ†æ•°çš„åŠ¨æ€åˆ¤æ–­
        text_scores = [score['text_score'] for score in relevance_scores]
        image_scores = [score['image_score'] for score in relevance_scores]
        sufficient_text = super().dynamic_retrieve_judge(text_scores, outlier_multiplier, top_mean_threshold, steepness_threshold)
        sufficient_image = super().dynamic_retrieve_judge(image_scores, outlier_multiplier, top_mean_threshold, steepness_threshold)
        return sufficient_text or sufficient_image
    
    def _has_sufficient_info_embed(self, query: str, docs: List[Dict[str, str]]):
        """åŸºäºåŠ¨æ€é˜ˆå€¼çš„ä¿¡æ¯å……åˆ†æ€§åˆ¤æ–­"""
        def document_converter(docs):
            return [Document(page_content=doc['text']) for doc in docs]
        
        index_dict = {doc['text']: i for i, doc in enumerate(docs)}
        
        retriver = EmbeddingMatcher(document_converter=document_converter)
        relevance_docs = retriver.match_docs(query, docs=docs, result_processor=self.rerank_index_processor)
        
        if not relevance_docs:
            return False, []
        
        relevance_scores = [doc['score'] for doc in relevance_docs]
        sufficient = self.dynamic_retrieve_judge(relevance_scores)
        
        ranked_indices = [index_dict[doc['text']] for doc in relevance_docs]
        
        return sufficient, ranked_indices


    def search_retrieval(self, data: dict, multi_inetent: False, retriever: EmbeddingMatcher):
        all_search_results = []
        original_query = deepcopy(data['query'])
        query = deepcopy(data['query'])
        
        embedding_topk = self.params['embedding_topk']
        rerank_topk = self.params['rerank_topk']
        
        query_list = [query]
        ranked_indices = None

        for iteration in range(self.max_iterations):
            print(f"ğŸ” Retrieval Iteration {iteration + 1}/N")
            print(f"ğŸ“ Query: {data['query']}")
            
            retrieval_list = retriever.match_docs(query, result_processor=self.result_processor)
            retrieval_list = self.llm_rerank(original_query, retrieval_list, self.reranker, rerank_topk)
            
            # ä½¿ç”¨å­—å…¸çš„æ–‡æœ¬å†…å®¹ä½œä¸ºå»é‡ä¾æ®
            seen_texts = set()
            unique_results = []
            for result in all_search_results + retrieval_list:
                if result['text'] not in seen_texts:
                    seen_texts.add(result['text'])
                    unique_results.append(result)
            all_search_results = unique_results
            
            all_search_results = sorted(all_search_results, key=lambda x: x['score'], reverse=True)
            all_search_results = all_search_results[:rerank_topk]
            
            # å¢åŠ å¯¹å›¾åƒå’Œæ–‡æœ¬åˆ†æ•°çš„åŠ¨æ€æƒé‡ç»„åˆ
            for doc in retrieval_list:
                doc['combined_score'] = doc['text_score'] * self.params.get('text_weight', 0.5) + \
                                        doc['image_score'] * self.params.get('image_weight', 0.5)
            retrieval_list = sorted(retrieval_list, key=lambda x: x['combined_score'], reverse=True)

            context = self._prepare_context(all_search_results)
            print(f"ğŸ“Š Context: {len(context)} characters | {len(retrieval_list)} results")
            
            # has_sufficient_info, ranked_indices = self._has_sufficient_information(original_query, context)
            # has_sufficient_info, ranked_indices = self._has_sufficient_info_embed(query, all_search_results)
            has_sufficient_info = self.dynamic_retrieve_judge([doc['score'] for doc in all_search_results])
            print([doc['score'] for doc in all_search_results])
            print(f"âœ… Sufficient Information: {'Yes' if has_sufficient_info else 'No'}")
            
            if has_sufficient_info:
                print("ğŸ¯ Search completed successfully")
                return all_search_results
            
            if iteration >= self.max_iterations - 1:
                print("âš ï¸ Max iterations reached. Generating answer with available information.")
                return all_search_results
            
            query = self._improve_query(json.dumps(query_list), context)
            query_list.append(query)
            if not query:
                print("ğŸ”„ Refined Query: None")
                break
            print(f"ğŸ”„ Refined Query: {query}")
            print("---")

        return all_search_results

    def _prepare_context(self, search_results: List[Dict[str, str]]) -> str:
        """Prepare context from search results."""
        return '\n'.join([f"{index+1}. {result['text']}" for index, result in enumerate(search_results)])

    def _has_sufficient_information(self, query: str, context: str):
        """
        æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿä¿¡æ¯å›ç­”æŸ¥è¯¢ï¼Œå¹¶è¿”å›æŒ‰ç›¸å…³æ€§æ’åºçš„æ–‡æ¡£ç´¢å¼•ã€‚
        
        Returns:
            tuple: (æ˜¯å¦æœ‰è¶³å¤Ÿä¿¡æ¯, æŒ‰ç›¸å…³æ€§æ’åºçš„æ–‡æ¡£ç´¢å¼•åˆ—è¡¨)
        """
        SYSTEM_MESSAGE_HAS_SUFFICIENT_INFO1 = dedent("""åˆ†ææœç´¢ç»“æœå¹¶ç¡®å®šæ˜¯å¦æœ‰è¶³å¤Ÿä¿¡æ¯å›ç­”ç”¨æˆ·çš„æŸ¥è¯¢ã€‚
        1. é¦–å…ˆåˆ¤æ–­æ˜¯å¦æœ‰è¶³å¤Ÿä¿¡æ¯å›ç­”é—®é¢˜
        2. ç„¶åæŒ‰ç…§ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§å¯¹æ¯ä¸ªæœç´¢ç»“æœè¿›è¡Œæ’åºï¼Œè¾“å‡ºæ’åºåçš„ç´¢å¼•å·ï¼ˆä»0å¼€å§‹ï¼‰,ä½ éœ€è¦å¯¹æ¯ä¸ªç»“æœè¿›è¡Œæ’åºï¼Œè¾“å‡ºå…¨éƒ¨æ’åºç»“æœï¼Œä¸è¦æœ‰é—æ¼
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        {
            "ranked_indices": [index1, index2, ...] # æŒ‰ç›¸å…³æ€§ä»é«˜åˆ°ä½æ’åºçš„ç´¢å¼•åˆ—è¡¨
            "sufficient": True/False,
        }""")
        
        SYSTEM_MESSAGE_HAS_SUFFICIENT_INFO2 = dedent("""åˆ†ææœç´¢ç»“æœçš„ç›¸å…³æ€§å’Œè´¨é‡ã€‚

        è¯·è¯„ä¼°æœç´¢ç»“æœå¹¶å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
        1. è¯„ä¼°æœç´¢ç»“æœä¸æŸ¥è¯¢çš„ç›¸å…³æ€§
        2. æŒ‰ç…§ç›¸å…³æ€§å¯¹æœç´¢ç»“æœè¿›è¡Œæ’åºï¼ˆè¾“å‡ºæ’åºåçš„ç´¢å¼•å·ï¼Œä»0å¼€å§‹ï¼‰
        3. å¦‚æœå‘ç°é«˜åº¦ç›¸å…³çš„ä¿¡æ¯ï¼ˆå³ä½¿ä¸å®Œæ•´ï¼‰ï¼Œä¹Ÿå¯ä»¥è€ƒè™‘åœæ­¢æœç´¢
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        {
            "sufficient": True/False,  # True: æ‰¾åˆ°ç›¸å…³åº¦é«˜çš„ç»“æœå¯ä»¥åœæ­¢æœç´¢; False: éœ€è¦ç»§ç»­æœç´¢
            "ranked_indices": [index1, index2, ...]  # æŒ‰ç›¸å…³æ€§æ’åºçš„ç´¢å¼•åˆ—è¡¨
        }""")
        

        messages = [
            {"role": "system", "content": SYSTEM_MESSAGE_HAS_SUFFICIENT_INFO1},
            {"role": "user", "content": f"Query: {query}\n\nSearch Results:\n{context}\n\nè¯·å¯¹ç»“æœæ’åºå¹¶åˆ†æä¿¡æ¯æ˜¯å¦è¶³å¤Ÿã€‚"}
        ]
        
        response_format = create_response_format({
            "ranked_indices": {
                "type": "array", 
                "description": "æŒ‰ç›¸å…³æ€§æ’åºçš„æ–‡æ¡£ç´¢å¼•åˆ—è¡¨",
                "items": {"type": "integer"},
            },
            "sufficient": {
                "type": "boolean", 
                "description": "True/False: True: æ‰¾åˆ°ç›¸å…³åº¦é«˜çš„ç»“æœå¯ä»¥åœæ­¢æœç´¢; False: éœ€è¦ç»§ç»­æœç´¢",                 
            }
        })
        response = AzureGPT4Chat(model_name="gpt-4o").chat_with_message_format(message_list=messages, response_format=response_format)
        print(f"ğŸ” Response: {response}")
        if not response:
            return False, []
        try:
            response = json.loads(response)
        except:
            return False, []
        print(f"ğŸ” Response: {response}")
        return response['sufficient'], response['ranked_indices']

    def _improve_query(self, query_history: str, context: str) -> str:
        """ä¼˜åŒ–æœç´¢æŸ¥è¯¢ä»¥å¢å¼ºå¤šç»´åº¦æ¢ç´¢èƒ½åŠ›"""
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        SYSTEM_MESSAGE_IMPROVE_QUERY = dedent("""æ‚¨æ˜¯æœç´¢ä¼˜åŒ–ä¸“å®¶ï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤ä¼˜åŒ–æŸ¥è¯¢ï¼š
        ã€æ­¥éª¤1ã€‘åˆ†æç°æœ‰ç»“æœ
        - è¯†åˆ«å·²è¦†ç›–çš„é¢†åŸŸå’Œç¼ºå¤±çš„ä¿¡æ¯
        - è¯„ä¼°ç»“æœçš„æ—¶é—´ç›¸å…³æ€§å’Œæ•°æ®å®Œæ•´æ€§

        current_time: {current_time}

        ã€æ­¥éª¤2ã€‘æ„å»ºçŸ¥è¯†å›¾è°±
        1. å®ä½“æå–ï¼š
           - è¯†åˆ«å…³é”®å®ä½“ï¼šäººç‰©ã€åœ°ç‚¹ã€äº‹ä»¶ã€æ¦‚å¿µç­‰
        2. å…³ç³»è¯†åˆ«ï¼š
           - è¯†åˆ«å®ä½“ä¹‹é—´çš„å…³ç³»ï¼šå› æœå…³ç³»ã€æ—¶é—´é¡ºåºã€å…³è”æ€§ç­‰
        3. ç”Ÿæˆä¸­é—´è¡¨ç¤ºï¼ˆå¿…é¡»è¾“å‡ºJSONï¼‰ï¼š
           {{
               "entities": ["å®ä½“1", "å®ä½“2", ...],
               "relations": [
                   {{"from": "å®ä½“1", "to": "å®ä½“2", "type": "å…³ç³»ç±»å‹"}},
                   ...
               ]
           }}
        ã€æ­¥éª¤3ã€‘ç”Ÿæˆä¼˜åŒ–åçš„æŸ¥è¯¢
        - åŸºäºåˆ†æç»“æœï¼Œç”Ÿæˆæ›´ç²¾å‡†çš„æœç´¢æŸ¥è¯¢
        """)

        messages = [
            {
                "role": "system", 
                "content": SYSTEM_MESSAGE_IMPROVE_QUERY.format(current_time=current_time)
            },
            {
                "role": "user",
                "content": dedent(f"""\
                ã€å†å²query listã€‘
                {query_history}

                ã€ç°æœ‰æœç´¢ç»“æœã€‘
                {context}

                ã€ä¼˜åŒ–è¦æ±‚ã€‘
                è¯·åŸºäºçŸ¥è¯†å›¾è°±åˆ†æï¼Œç”Ÿæˆæ›´ç²¾å‡†çš„æœç´¢æŸ¥è¯¢ï¼š""")
            }
        ]
        # æ·»åŠ å“åº”æ ¼å¼çº¦æŸ
        response_format = create_response_format({
            "entities": {
                "type": "array",
                "description": "å®ä½“åˆ—è¡¨",
                "items": {"type": "string"},
            },
            "relations": {
                "type": "array",
                "description": "å…³ç³»åˆ—è¡¨",
                "items": {
                    "type": "object",
                    "properties": {
                        "from": {"type": "string"},
                        "to": {"type": "string"},
                        "type": {"type": "string"}
                    },
                    "required": ["from", "to", "type"],
                    "additionalProperties": False
                }
            },
            "improved_query": {
                "type": "string",
                "description": "ä¼˜åŒ–åçš„æœç´¢æŸ¥è¯¢è¯­å¥ï¼ŒåŒ…å«å…·ä½“æ—¶é—´èŒƒå›´å’Œä¸“ä¸šæœ¯è¯­"
            }
        })

        response = AzureGPT4Chat().chat_with_message_format(
            message_list=messages,
            response_format=response_format
        )
        print(f"====ğŸ” improved_query: {response}")
        # è§£ææ ¼å¼åŒ–çš„å“åº”
        try:
            return json.loads(response).get("improved_query", json.loads(query_history)[-1])
        except Exception as e:
            print(f"==== error: {e}")
            return response or json.loads(query_history)[-1]
