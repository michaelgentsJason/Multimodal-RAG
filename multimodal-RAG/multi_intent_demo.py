#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import json
import time
import argparse
from pathlib import Path
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from dotenv import load_dotenv
from FlagEmbedding import FlagReranker
import torch
from PIL import Image
import subprocess
import pandas as pd
from collections import defaultdict
from beam_search_module import BeamSearchWrapper
import logging
import os
from dotenv import load_dotenv

# åˆ›å»ºæ—¥å¿—ç›®å½•
log_dir = Path("DeepRAG_Multimodal/log")
log_dir.mkdir(exist_ok=True)
log_file = log_dir / "multi_intent_demo.log"

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(str(log_file), mode='w', encoding='utf-8'),  # ä½¿ç”¨'w'æ¨¡å¼æ¸…ç©ºæ—¥å¿—
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# æ·»åŠ å¿…è¦çš„è·¯å¾„
sys.path.append("multimodal-RAG/DeepRAG_Multimodal/deep_retrieve")
# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv("D:\Desktop\multimodal-RAG\multimodal-RAG\DeepRAG_Multimodal\configs\.env")

# å¯¼å…¥å¿…è¦çš„åº“
from DeepRAG_Multimodal.deep_retrieve.ming.deepsearch_optimize_ming import DeepSearch_Beta
from DeepRAG_Multimodal.deep_retrieve.retriever_multimodal_bge import RetrieverConfig, MultimodalMatcher


class MultiIntentDemo:
    """å¤šæ„å›¾æ‹†è§£æ•ˆæœæ¼”ç¤ºç±»"""

    def __init__(self):
        """åˆå§‹åŒ–æ¼”ç¤ºå™¨"""
        self.config = self.load_config()
        os.makedirs(self.config['results_dir'], exist_ok=True)
        os.makedirs(self.config['vis_dir'], exist_ok=True)

        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False

        self.setup_models()

    def load_config(self):
        """åŠ è½½é…ç½®"""
        config = {
            # è·¯å¾„é…ç½®
            'test_data_path': r'D:\Desktop\colpali_longdoc\picked_LongDoc\selected_LongDocURL_public_with_subtask_category.jsonl',
            'pdf_base_dir': r'D:\Desktop\colpali_longdoc\picked_LongDoc',
            'results_dir': './demo_results',
            'vis_dir': './demo_results/visualizations',

            # é‡‡æ ·é…ç½® - åªæµ‹è¯•2æ¡æ•°æ®
            'sample_size': 1,
            'debug': True,

            # æ£€ç´¢é…ç½®
            'max_iterations': 2,
            'embedding_topk': 8,
            'rerank_topk': 5,
            'text_weight': 1.0,
            'image_weight': 0.0,

            # æ¨¡å‹é…ç½®
            'mm_model_name': "vidore/colqwen2.5-v0.2",
            'mm_processor_name': "vidore/colqwen2.5-v0.1",
            'bge_model_name': "BAAI/bge-large-en-v1.5",
            'device': 'cuda:0',
            'batch_size': 2,
            'retrieval_mode': 'text_only',  # ä¸“æ³¨äºæ–‡æœ¬æ£€ç´¢
            'ocr_method': 'paddleocr',

            # ç¦ç”¨Vespa
            'use_vespa': False,

            # ğŸ¯ æ·»åŠ  Beam Search é…ç½®
            'enable_beam_search': True,  # ä¸»å¼€å…³
            'beam_width': 3,  # beamå®½åº¦
            'beam_debug': True,  # è°ƒè¯•æ¨¡å¼
        }
        return config

    def setup_models(self):
        """åˆå§‹åŒ–æ£€ç´¢æ¨¡å‹"""
        logger.info("ğŸš€ åˆå§‹åŒ–å¤šæ„å›¾æ£€ç´¢æ¨¡å‹...")
        import torch
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        logger.info(f"ğŸ® ä½¿ç”¨è®¾å¤‡: {device}")

        self.reranker = FlagReranker(
            model_name_or_path="BAAI/bge-reranker-large",
            use_fp16=True,  # å¯ç”¨FP16åŠ é€Ÿ
            device=device  # ğŸ”¥ å¼ºåˆ¶ä½¿ç”¨GPU
        )

        # éªŒè¯rerankerè®¾å¤‡
        logger.info(f"ğŸ“ Rerankerè®¾å¤‡: {next(self.reranker.model.parameters()).device}")

        # åˆå§‹åŒ–é‡æ’åºå™¨
        self.reranker = FlagReranker(model_name_or_path="BAAI/bge-reranker-large")

        # åˆå§‹åŒ–å¤šæ¨¡æ€åŒ¹é…å™¨é…ç½®
        retriever_config = RetrieverConfig(
            model_name=self.config['mm_model_name'],
            processor_name=self.config['mm_processor_name'],
            bge_model_name=self.config['bge_model_name'],
            device=self.config['device'],
            use_fp16=True,
            batch_size=self.config['batch_size'],
            mode=self.config['retrieval_mode'],
            ocr_method=self.config['ocr_method']
        )

        # ä½¿ç”¨æ ‡å‡†å¤šæ¨¡æ€åŒ¹é…å™¨ï¼ˆä¸ä½¿ç”¨Vespaï¼‰
        self.mm_matcher = MultimodalMatcher(
            config=retriever_config,
            embedding_weight=self.config['text_weight'],
            topk=self.config['rerank_topk']
        )
        logger.info("âœ… å·²åˆå§‹åŒ–æ ‡å‡†å¤šæ¨¡æ€åŒ¹é…å™¨")

        # åˆå§‹åŒ–åŸºç¡€å¤šæ„å›¾æ£€ç´¢å™¨
        base_multi_intent_search = DeepSearch_Beta(
            max_iterations=self.config['max_iterations'],
            reranker=self.reranker,
            params={
                "embedding_topk": self.config['embedding_topk'],
                "rerank_topk": self.config['rerank_topk'],
                "text_weight": self.config['text_weight'],
                "image_weight": self.config['image_weight']
            }
        )

        # ğŸ¯ ç”¨Beam SearchåŒ…è£…å™¨åŒ…è£…ï¼ˆå¯å¼€å…³æ§åˆ¶ï¼‰
        self.multi_intent_search = BeamSearchWrapper(
            base_retriever=base_multi_intent_search,
            matcher=self.mm_matcher,
            reranker=self.reranker,
            enable_beam_search=True,  # ğŸ”¥ åœ¨è¿™é‡Œæ§åˆ¶å¼€å…³ï¼
            beam_width=3,
            debug_mode=True
        )

        # åˆå§‹åŒ–å•æ„å›¾æ£€ç´¢å™¨ï¼ˆç¦ç”¨æ„å›¾æ‹†è§£ï¼‰
        self.single_intent_search = DeepSearch_Beta(
            max_iterations=self.config['max_iterations'],
            reranker=self.reranker,
            params={
                "embedding_topk": self.config['embedding_topk'],
                "rerank_topk": self.config['rerank_topk'],
                "text_weight": self.config['text_weight'],
                "image_weight": self.config['image_weight']
            }
        )

        # ç¦ç”¨å•æ„å›¾æ£€ç´¢å™¨çš„æ„å›¾æ‹†è§£åŠŸèƒ½
        self.single_intent_search._split_query_intent = lambda query: [query]
        self.single_intent_search._split_query_intent_exist = lambda query: [query]

        logger.info("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")





    def load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        allowed_doc_nos = [
            '4046173.pdf', '4176503.pdf', '4057524.pdf', '4064501.pdf', '4057121.pdf'
        ]

        logger.info(f"ğŸ“š åŠ è½½æµ‹è¯•æ•°æ®: {self.config['test_data_path']}")
        test_data = []

        with open(self.config['test_data_path'], 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    item = json.loads(line)
                    if item.get("pdf_path") in allowed_doc_nos:
                        test_data.append(item)

        # åªå–å‰2æ¡æ•°æ®
        test_data = test_data[:self.config['sample_size']]
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(test_data)} æ¡æµ‹è¯•æ•°æ®")

        # æ‰“å°æµ‹è¯•æ•°æ®åŸºæœ¬ä¿¡æ¯
        for i, data in enumerate(test_data):
            logger.info(
                f"ğŸ“„ æµ‹è¯•æ•°æ® {i + 1}: {data.get('pdf_path', 'Unknown')} - {data.get('question', 'No question')[:50]}...")

        return test_data

    def process_single_document(self, doc_data):
        """å¤„ç†å•ä¸ªæ–‡æ¡£ï¼Œç›´æ¥ä½¿ç”¨OCRç»“æœ"""
        documents = []

        # è·å–é¢„å¤„ç†çš„OCRç»“æœ
        ocr_file = os.path.join(
            self.config['pdf_base_dir'],
            f"{self.config['ocr_method']}_save",
            f"{os.path.basename(doc_data['pdf_path']).replace('.pdf', '.json')}"
        )

        # è¯»å–é¢„å¤„ç†çš„æ–‡æœ¬æ•°æ®
        if os.path.exists(ocr_file):
            with open(ocr_file, 'r', encoding='utf-8') as f:
                loaded_data = json.load(f)
            logger.info(f"ğŸ“– æˆåŠŸè¯»å–OCRæ–‡ä»¶: {ocr_file}")
        else:
            logger.warning(f"âš ï¸ æ‰¾ä¸åˆ°OCRæ–‡ä»¶: {ocr_file}")
            # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
            logger.info(f"ğŸ”§ ä¸º {doc_data['pdf_path']} åˆ›å»ºæ¨¡æ‹ŸOCRæ•°æ®")
            loaded_data = {}
            for i in range(5):
                loaded_data[
                    f"Page_{i + 1}"] = f"è¿™æ˜¯ {doc_data['pdf_path']} ç¬¬{i + 1}é¡µçš„æ¨¡æ‹Ÿå†…å®¹ï¼ŒåŒ…å«æµ‹è¯•æ–‡æœ¬ç”¨äºæ£€ç´¢å®éªŒã€‚"

        # ä¸ºæ¯ä¸€é¡µåˆ›å»ºæ–‡æ¡£å¯¹è±¡
        for idx, (page_key, page_text) in enumerate(loaded_data.items()):
            documents.append({
                "text": page_text if page_text.strip() else f"ç¬¬{idx + 1}é¡µå†…å®¹",
                "image": None,
                "metadata": {
                    "page_index": idx + 1,
                    "pdf_path": doc_data.get("pdf_path", "")
                }
            })

        logger.info(f"ğŸ“‘ æˆåŠŸåˆ›å»º {len(documents)} ä¸ªæ–‡æ¡£å¯¹è±¡")
        return documents

    def demonstrate_intent_decomposition(self):
        """æ¼”ç¤ºå¤šæ„å›¾æ‹†è§£æ•ˆæœ"""

        logger.info("ğŸ¯ å¼€å§‹å¤šæ„å›¾æ‹†è§£æ•ˆæœæ¼”ç¤º...")
        test_data = self.load_test_data()
        results = []

        for idx, doc_data in enumerate(test_data):
            logger.info(f"\n{'=' * 60}")
            logger.info(f"ğŸ” å¤„ç†æ–‡æ¡£ {idx + 1}/{len(test_data)}: {doc_data.get('pdf_path', 'Unknown')}")

            query = doc_data.get("question", "")
            evidence_pages = doc_data.get("evidence_pages", [])

            logger.info(f"â“ åŸå§‹æŸ¥è¯¢: {query}")
            logger.info(f"ğŸ“‹ è¯æ®é¡µé¢: {evidence_pages}")

            # å¤„ç†æ–‡æ¡£
            document_pages = self.process_single_document(doc_data)
            if not document_pages:
                logger.warning(f"âš ï¸ è·³è¿‡æ–‡æ¡£: æ— æœ‰æ•ˆå†…å®¹")
                continue

            data = {
                "query": query,
                "documents": document_pages
            }

            # æ¼”ç¤ºæ„å›¾æ‹†è§£è¿‡ç¨‹
            intent_decomposition_result = self.analyze_intent_decomposition(query)

            # æ‰§è¡Œå•æ„å›¾æ£€ç´¢
            logger.info(f"\nğŸ” æ‰§è¡Œå•æ„å›¾æ£€ç´¢...")
            single_start = time.time()
            single_results = self.single_intent_search.search_retrieval(data, retriever=self.mm_matcher)
            single_elapsed = time.time() - single_start
            logger.info(f"â±ï¸ å•æ„å›¾æ£€ç´¢è€—æ—¶: {single_elapsed:.2f}ç§’")

            # æ‰§è¡Œå¤šæ„å›¾æ£€ç´¢
            logger.info(f"\nğŸ¯ æ‰§è¡Œå¤šæ„å›¾æ£€ç´¢...")
            multi_start = time.time()
            multi_results = self.multi_intent_search.search_retrieval(data, retriever=self.mm_matcher)
            multi_elapsed = time.time() - multi_start
            logger.info(f"â±ï¸ å¤šæ„å›¾æ£€ç´¢è€—æ—¶: {multi_elapsed:.2f}ç§’")

            # åˆ†ææ£€ç´¢ç»“æœ
            single_analysis = self.analyze_retrieval_results(single_results, evidence_pages, "å•æ„å›¾")
            multi_analysis = self.analyze_retrieval_results(multi_results, evidence_pages, "å¤šæ„å›¾")

            # è®°å½•ç»“æœ
            result = {
                "doc_id": doc_data.get("doc_no", ""),
                "pdf_path": doc_data.get("pdf_path", ""),
                "query": query,
                "evidence_pages": evidence_pages,
                "intent_decomposition": intent_decomposition_result,
                "single_intent": {
                    **single_analysis,
                    "retrieval_time": single_elapsed,
                    "results": single_results[:5]  # åªä¿å­˜å‰5ä¸ªç»“æœ
                },
                "multi_intent": {
                    **multi_analysis,
                    "retrieval_time": multi_elapsed,
                    "results": multi_results[:5]  # åªä¿å­˜å‰5ä¸ªç»“æœ
                }
            }

            results.append(result)

            # ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºè¯¦ç»†åˆ†æ
            self.create_detailed_analysis(result, idx + 1)

        # ä¿å­˜æ•´ä½“ç»“æœ
        result_file = os.path.join(self.config['results_dir'], 'intent_decomposition_demo.json')
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        # åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–
        self.create_comparison_visualizations(results)

        logger.info(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {self.config['results_dir']}")
        return results

    def analyze_intent_decomposition(self, query):
        """åˆ†ææ„å›¾æ‹†è§£è¿‡ç¨‹"""
        logger.info(f"\nğŸ§  åˆ†ææŸ¥è¯¢æ„å›¾æ‹†è§£è¿‡ç¨‹...")

        # è°ƒç”¨å¤šæ„å›¾æ£€ç´¢å™¨çš„æ„å›¾æ‹†è§£æ–¹æ³•
        intent_queries = self.multi_intent_search._split_query_intent(query)

        logger.info(f"ğŸ“ åŸå§‹æŸ¥è¯¢: {query}")
        logger.info(f"ğŸ¯ æ‹†è§£å‡º {len(intent_queries)} ä¸ªå­æ„å›¾:")

        for i, intent in enumerate(intent_queries, 1):
            logger.info(f"   {i}. {intent}")

        # å¦‚æœæ‹†è§£ç»“æœåªæœ‰åŸæŸ¥è¯¢ï¼Œè¯´æ˜LLMè®¤ä¸ºä¸éœ€è¦æ‹†è§£
        if len(intent_queries) == 1 and intent_queries[0] == query:
            logger.info("   ğŸ’¡ LLMåˆ¤æ–­æ­¤æŸ¥è¯¢ä¸éœ€è¦æ‹†è§£")

        # åˆ†ææ„å›¾ç±»å‹å’Œè¦†ç›–åº¦
        intent_analysis = {
            "original_query": query,
            "decomposed_intents": intent_queries,
            "intent_count": len(intent_queries),
            "coverage_analysis": self.analyze_intent_coverage(query, intent_queries)
        }

        return intent_analysis

    def analyze_intent_coverage(self, original_query, intent_queries):
        """åˆ†ææ„å›¾è¦†ç›–åº¦"""
        # ç®€å•çš„å…³é”®è¯è¦†ç›–åˆ†æ
        original_words = set(original_query.lower().split())

        coverage_stats = {
            "original_word_count": len(original_words),
            "intent_coverage": []
        }

        for intent in intent_queries:
            intent_words = set(intent.lower().split())
            overlap = original_words.intersection(intent_words)
            coverage = len(overlap) / len(original_words) if original_words else 0

            coverage_stats["intent_coverage"].append({
                "intent": intent,
                "word_overlap": len(overlap),
                "coverage_ratio": coverage,
                "new_words": list(intent_words - original_words)
            })

        return coverage_stats

    def analyze_retrieval_results(self, results, evidence_pages, method_name):
        """åˆ†ææ£€ç´¢ç»“æœ"""
        retrieved_pages = set()
        for result in results:
            if 'page' in result and result['page'] is not None:
                retrieved_pages.add(result['page'])
            elif 'metadata' in result and 'page_index' in result['metadata']:
                retrieved_pages.add(result['metadata']['page_index'])

        evidence_set = set(evidence_pages)
        correct_pages = evidence_set.intersection(retrieved_pages)

        recall = len(correct_pages) / len(evidence_set) if evidence_set else 0
        precision = len(correct_pages) / len(retrieved_pages) if retrieved_pages else 0
        f1 = 2 * recall * precision / (recall + precision) if (recall + precision) > 0 else 0

        logger.info(f"ğŸ“Š {method_name}æ£€ç´¢ç»“æœåˆ†æ:")
        logger.info(f"   ğŸ¯ æ£€ç´¢åˆ°é¡µé¢: {sorted(list(retrieved_pages))}")
        logger.info(f"   âœ… æ­£ç¡®é¡µé¢: {sorted(list(correct_pages))}")
        logger.info(f"   ğŸ“ˆ å¬å›ç‡: {recall:.4f}")
        logger.info(f"   ğŸ“ˆ ç²¾ç¡®ç‡: {precision:.4f}")
        logger.info(f"   ğŸ“ˆ F1å€¼: {f1:.4f}")

        return {
            "retrieved_pages": list(retrieved_pages),
            "correct_pages": list(correct_pages),
            "recall": recall,
            "precision": precision,
            "f1": f1,
            "success": len(correct_pages) == len(evidence_set)
        }

    def create_detailed_analysis(self, result, doc_index):
        """ä¸ºå•ä¸ªæ–‡æ¡£åˆ›å»ºè¯¦ç»†åˆ†æå›¾è¡¨"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'æ–‡æ¡£ {doc_index}: å¤šæ„å›¾æ‹†è§£è¯¦ç»†åˆ†æ\næŸ¥è¯¢: {result["query"][:50]}...', fontsize=14,
                     fontweight='bold')

        # 1. æ„å›¾æ‹†è§£å¯è§†åŒ–
        ax1 = axes[0, 0]
        intent_data = result["intent_decomposition"]
        intents = intent_data["decomposed_intents"]

        # åˆ›å»ºæ„å›¾é•¿åº¦æ¡å½¢å›¾
        intent_lengths = [len(intent.split()) for intent in intents]
        bars = ax1.bar(range(len(intents)), intent_lengths, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
        ax1.set_title('æ‹†è§£æ„å›¾é•¿åº¦åˆ†å¸ƒ', fontweight='bold')
        ax1.set_xlabel('æ„å›¾ç¼–å·')
        ax1.set_ylabel('è¯æ•°')
        ax1.set_xticks(range(len(intents)))
        ax1.set_xticklabels([f'æ„å›¾{i + 1}' for i in range(len(intents))])

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, length in zip(bars, intent_lengths):
            ax1.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,
                     str(length), ha='center', va='bottom')

        # 2. æ£€ç´¢æ€§èƒ½å¯¹æ¯”
        ax2 = axes[0, 1]
        metrics = ['å¬å›ç‡', 'ç²¾ç¡®ç‡', 'F1å€¼']
        single_scores = [result["single_intent"]["recall"], result["single_intent"]["precision"],
                         result["single_intent"]["f1"]]
        multi_scores = [result["multi_intent"]["recall"], result["multi_intent"]["precision"],
                        result["multi_intent"]["f1"]]

        x = np.arange(len(metrics))
        width = 0.35

        bars1 = ax2.bar(x - width / 2, single_scores, width, label='å•æ„å›¾', color='#FF6B6B', alpha=0.8)
        bars2 = ax2.bar(x + width / 2, multi_scores, width, label='å¤šæ„å›¾', color='#4ECDC4', alpha=0.8)

        ax2.set_title('æ£€ç´¢æ€§èƒ½å¯¹æ¯”', fontweight='bold')
        ax2.set_ylabel('åˆ†æ•°')
        ax2.set_xticks(x)
        ax2.set_xticklabels(metrics)
        ax2.legend()
        ax2.set_ylim(0, 1)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width() / 2, height + 0.01,
                         f'{height:.3f}', ha='center', va='bottom', fontsize=9)

        # 3. æ£€ç´¢æ—¶é—´å¯¹æ¯”
        ax3 = axes[1, 0]
        methods = ['å•æ„å›¾', 'å¤šæ„å›¾']
        times = [result["single_intent"]["retrieval_time"], result["multi_intent"]["retrieval_time"]]

        bars = ax3.bar(methods, times, color=['#FF6B6B', '#4ECDC4'], alpha=0.8)
        ax3.set_title('æ£€ç´¢æ—¶é—´å¯¹æ¯”', fontweight='bold')
        ax3.set_ylabel('æ—¶é—´ (ç§’)')

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, time_val in zip(bars, times):
            ax3.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                     f'{time_val:.2f}s', ha='center', va='bottom')

        # 4. é¡µé¢æ£€ç´¢å‡†ç¡®æ€§
        ax4 = axes[1, 1]
        evidence_pages = set(result["evidence_pages"])
        single_pages = set(result["single_intent"]["retrieved_pages"])
        multi_pages = set(result["multi_intent"]["retrieved_pages"])

        # åˆ›å»ºç»´æ©å›¾å¼çš„åˆ†æ
        categories = ['ä»…å•æ„å›¾', 'ä»…å¤šæ„å›¾', 'ä¸¤è€…å…±åŒ', 'é—æ¼é¡µé¢']
        single_only = single_pages - multi_pages - evidence_pages
        multi_only = multi_pages - single_pages - evidence_pages
        both_correct = (single_pages & multi_pages) & evidence_pages
        missed = evidence_pages - (single_pages | multi_pages)

        counts = [len(single_only), len(multi_only), len(both_correct), len(missed)]
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']

        wedges, texts, autotexts = ax4.pie(counts, labels=categories, colors=colors, autopct='%1.0f',
                                           startangle=90)
        ax4.set_title('é¡µé¢æ£€ç´¢åˆ†å¸ƒ', fontweight='bold')

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        chart_file = os.path.join(self.config['vis_dir'], f'doc_{doc_index}_detailed_analysis.png')
        plt.savefig(chart_file, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"ğŸ“Š è¯¦ç»†åˆ†æå›¾è¡¨å·²ä¿å­˜: {chart_file}")

        # åˆ›å»ºæ„å›¾æ‹†è§£æ–‡æœ¬åˆ†æ
        self.create_intent_text_analysis(result, doc_index)

    def create_intent_text_analysis(self, result, doc_index):
        """åˆ›å»ºæ„å›¾æ‹†è§£çš„æ–‡æœ¬åˆ†ææŠ¥å‘Š"""
        report_file = os.path.join(self.config['vis_dir'], f'doc_{doc_index}_intent_analysis.txt')

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"æ–‡æ¡£ {doc_index} å¤šæ„å›¾æ‹†è§£åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")

            f.write(f"ğŸ“„ æ–‡æ¡£: {result['pdf_path']}\n")
            f.write(f"â“ åŸå§‹æŸ¥è¯¢: {result['query']}\n")
            f.write(f"ğŸ“‹ è¯æ®é¡µé¢: {result['evidence_pages']}\n\n")

            f.write("ğŸ¯ æ„å›¾æ‹†è§£ç»“æœ:\n")
            f.write("-" * 30 + "\n")
            intent_data = result["intent_decomposition"]
            for i, intent in enumerate(intent_data["decomposed_intents"], 1):
                f.write(f"{i}. {intent}\n")

            f.write(f"\nğŸ“Š æ‹†è§£ç»Ÿè®¡:\n")
            f.write(f"   - åŸå§‹æŸ¥è¯¢è¯æ•°: {len(result['query'].split())}\n")
            f.write(f"   - æ‹†è§£æ„å›¾æ•°é‡: {intent_data['intent_count']}\n")

            f.write(f"\nğŸ” æ£€ç´¢ç»“æœå¯¹æ¯”:\n")
            f.write("-" * 30 + "\n")
            f.write(f"å•æ„å›¾æ£€ç´¢:\n")
            f.write(f"   - æ£€ç´¢é¡µé¢: {result['single_intent']['retrieved_pages']}\n")
            f.write(f"   - æ­£ç¡®é¡µé¢: {result['single_intent']['correct_pages']}\n")
            f.write(f"   - F1å€¼: {result['single_intent']['f1']:.4f}\n")
            f.write(f"   - ç”¨æ—¶: {result['single_intent']['retrieval_time']:.2f}ç§’\n\n")

            f.write(f"å¤šæ„å›¾æ£€ç´¢:\n")
            f.write(f"   - æ£€ç´¢é¡µé¢: {result['multi_intent']['retrieved_pages']}\n")
            f.write(f"   - æ­£ç¡®é¡µé¢: {result['multi_intent']['correct_pages']}\n")
            f.write(f"   - F1å€¼: {result['multi_intent']['f1']:.4f}\n")
            f.write(f"   - ç”¨æ—¶: {result['multi_intent']['retrieval_time']:.2f}ç§’\n\n")

            f.write(f"ğŸ“ˆ æ€§èƒ½æå‡:\n")
            f.write("-" * 30 + "\n")
            recall_diff = result['multi_intent']['recall'] - result['single_intent']['recall']
            precision_diff = result['multi_intent']['precision'] - result['single_intent']['precision']
            f1_diff = result['multi_intent']['f1'] - result['single_intent']['f1']

            f.write(f"   - å¬å›ç‡å˜åŒ–: {recall_diff:+.4f}\n")
            f.write(f"   - ç²¾ç¡®ç‡å˜åŒ–: {precision_diff:+.4f}\n")
            f.write(f"   - F1å€¼å˜åŒ–: {f1_diff:+.4f}\n")

            if f1_diff > 0:
                f.write(f"   âœ… å¤šæ„å›¾æ£€ç´¢è¡¨ç°æ›´å¥½\n")
            elif f1_diff < 0:
                f.write(f"   âŒ å•æ„å›¾æ£€ç´¢è¡¨ç°æ›´å¥½\n")
            else:
                f.write(f"   â– ä¸¤ç§æ–¹æ³•è¡¨ç°ç›¸å½“\n")

        logger.info(f"ğŸ“ æ„å›¾åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    def create_comparison_visualizations(self, results):
        """åˆ›å»ºæ•´ä½“å¯¹æ¯”å¯è§†åŒ–"""
        logger.info(f"ğŸ“Š åˆ›å»ºæ•´ä½“å¯¹æ¯”å¯è§†åŒ–...")

        # 1. æ•´ä½“æ€§èƒ½å¯¹æ¯”å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('å¤šæ„å›¾æ‹†è§£æ•´ä½“æ•ˆæœåˆ†æ', fontsize=16, fontweight='bold')

        # æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
        ax1 = axes[0, 0]
        metrics = ['å¬å›ç‡', 'ç²¾ç¡®ç‡', 'F1å€¼']

        single_avg = [
            np.mean([r["single_intent"]["recall"] for r in results]),
            np.mean([r["single_intent"]["precision"] for r in results]),
            np.mean([r["single_intent"]["f1"] for r in results])
        ]

        multi_avg = [
            np.mean([r["multi_intent"]["recall"] for r in results]),
            np.mean([r["multi_intent"]["precision"] for r in results]),
            np.mean([r["multi_intent"]["f1"] for r in results])
        ]

        x = np.arange(len(metrics))
        width = 0.35

        bars1 = ax1.bar(x - width / 2, single_avg, width, label='å•æ„å›¾', color='#FF6B6B', alpha=0.8)
        bars2 = ax1.bar(x + width / 2, multi_avg, width, label='å¤šæ„å›¾', color='#4ECDC4', alpha=0.8)

        ax1.set_title('å¹³å‡æ€§èƒ½å¯¹æ¯”', fontweight='bold')
        ax1.set_ylabel('åˆ†æ•°')
        ax1.set_xticks(x)
        ax1.set_xticklabels(metrics)
        ax1.legend()
        ax1.set_ylim(0, 1)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width() / 2, height + 0.01,
                         f'{height:.3f}', ha='center', va='bottom', fontsize=9)

        # 2. æ—¶é—´å¯¹æ¯”
        ax2 = axes[0, 1]
        single_times = [r["single_intent"]["retrieval_time"] for r in results]
        multi_times = [r["multi_intent"]["retrieval_time"] for r in results]

        methods = ['å•æ„å›¾', 'å¤šæ„å›¾']
        avg_times = [np.mean(single_times), np.mean(multi_times)]

        bars = ax2.bar(methods, avg_times, color=['#FF6B6B', '#4ECDC4'], alpha=0.8)
        ax2.set_title('å¹³å‡æ£€ç´¢æ—¶é—´å¯¹æ¯”', fontweight='bold')
        ax2.set_ylabel('æ—¶é—´ (ç§’)')

        for bar, time_val in zip(bars, avg_times):
            ax2.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                     f'{time_val:.2f}s', ha='center', va='bottom')

        # 3. æ„å›¾æ‹†è§£ç»Ÿè®¡
        ax3 = axes[1, 0]
        intent_counts = [r["intent_decomposition"]["intent_count"] for r in results]

        ax3.hist(intent_counts, bins=range(1, max(intent_counts) + 2), alpha=0.7, color='#45B7D1', edgecolor='black')
        ax3.set_title('æ„å›¾æ‹†è§£æ•°é‡åˆ†å¸ƒ', fontweight='bold')
        ax3.set_xlabel('æ‹†è§£æ„å›¾æ•°é‡')
        ax3.set_ylabel('æ–‡æ¡£æ•°é‡')

        # 4. æˆåŠŸç‡å¯¹æ¯”
        ax4 = axes[1, 1]
        single_success = sum(1 for r in results if r["single_intent"]["success"])
        multi_success = sum(1 for r in results if r["multi_intent"]["success"])

        categories = ['å•æ„å›¾æˆåŠŸ', 'å¤šæ„å›¾æˆåŠŸ']
        success_counts = [single_success, multi_success]

        bars = ax4.bar(categories, success_counts, color=['#FF6B6B', '#4ECDC4'], alpha=0.8)
        ax4.set_title('å®Œå…¨æˆåŠŸæ¡ˆä¾‹æ•°', fontweight='bold')
        ax4.set_ylabel('æˆåŠŸæ•°é‡')

        for bar, count in zip(bars, success_counts):
            ax4.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.05,
                     str(count), ha='center', va='bottom')

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        overview_file = os.path.join(self.config['vis_dir'], 'overall_comparison.png')
        plt.savefig(overview_file, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"ğŸ“Š æ•´ä½“å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {overview_file}")

        # åˆ›å»ºæ€»ç»“æŠ¥å‘Š
        self.create_summary_report(results)

    def create_summary_report(self, results):
        """åˆ›å»ºæ€»ç»“æŠ¥å‘Š"""
        report_file = os.path.join(self.config['results_dir'], 'summary_report.txt')

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("å¤šæ„å›¾æ‹†è§£æ•ˆæœæ¼”ç¤ºæ€»ç»“æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")

            f.write(f"ğŸ“Š æµ‹è¯•æ¦‚å†µ:\n")
            f.write(f"   - æµ‹è¯•æ–‡æ¡£æ•°é‡: {len(results)}\n")
            f.write(f"   - æ£€ç´¢æ¨¡å¼: {self.config['retrieval_mode']}\n")
            f.write(f"   - OCRæ–¹æ³•: {self.config['ocr_method']}\n\n")

            # è®¡ç®—å¹³å‡æŒ‡æ ‡
            single_avg_recall = np.mean([r["single_intent"]["recall"] for r in results])
            multi_avg_recall = np.mean([r["multi_intent"]["recall"] for r in results])
            single_avg_precision = np.mean([r["single_intent"]["precision"] for r in results])
            multi_avg_precision = np.mean([r["multi_intent"]["precision"] for r in results])
            single_avg_f1 = np.mean([r["single_intent"]["f1"] for r in results])
            multi_avg_f1 = np.mean([r["multi_intent"]["f1"] for r in results])

            f.write(f"ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”:\n")
            f.write(f"   å•æ„å›¾æ£€ç´¢:\n")
            f.write(f"     - å¹³å‡å¬å›ç‡: {single_avg_recall:.4f}\n")
            f.write(f"     - å¹³å‡ç²¾ç¡®ç‡: {single_avg_precision:.4f}\n")
            f.write(f"     - å¹³å‡F1å€¼: {single_avg_f1:.4f}\n")
            f.write(f"   å¤šæ„å›¾æ£€ç´¢:\n")
            f.write(f"     - å¹³å‡å¬å›ç‡: {multi_avg_recall:.4f}\n")
            f.write(f"     - å¹³å‡ç²¾ç¡®ç‡: {multi_avg_precision:.4f}\n")
            f.write(f"     - å¹³å‡F1å€¼: {multi_avg_f1:.4f}\n\n")

            f.write(f"ğŸš€ æ€§èƒ½æå‡:\n")
            f.write(f"   - å¬å›ç‡æå‡: {multi_avg_recall - single_avg_recall:+.4f}\n")
            f.write(f"   - ç²¾ç¡®ç‡æå‡: {multi_avg_precision - single_avg_precision:+.4f}\n")
            f.write(f"   - F1å€¼æå‡: {multi_avg_f1 - single_avg_f1:+.4f}\n\n")

            # æ—¶é—´åˆ†æ
            single_avg_time = np.mean([r["single_intent"]["retrieval_time"] for r in results])
            multi_avg_time = np.mean([r["multi_intent"]["retrieval_time"] for r in results])

            f.write(f"â±ï¸ æ—¶é—´æ•ˆç‡:\n")
            f.write(f"   - å•æ„å›¾å¹³å‡æ—¶é—´: {single_avg_time:.2f}ç§’\n")
            f.write(f"   - å¤šæ„å›¾å¹³å‡æ—¶é—´: {multi_avg_time:.2f}ç§’\n")
            f.write(f"   - æ—¶é—´å¢åŠ : {multi_avg_time - single_avg_time:+.2f}ç§’\n\n")

            # æˆåŠŸç‡åˆ†æ
            single_success = sum(1 for r in results if r["single_intent"]["success"])
            multi_success = sum(1 for r in results if r["multi_intent"]["success"])

            f.write(f"ğŸ¯ æˆåŠŸç‡åˆ†æ:\n")
            f.write(
                f"   - å•æ„å›¾å®Œå…¨æˆåŠŸ: {single_success}/{len(results)} ({single_success / len(results) * 100:.1f}%)\n")
            f.write(
                f"   - å¤šæ„å›¾å®Œå…¨æˆåŠŸ: {multi_success}/{len(results)} ({multi_success / len(results) * 100:.1f}%)\n\n")

            # æ„å›¾æ‹†è§£åˆ†æ
            intent_counts = [r["intent_decomposition"]["intent_count"] for r in results]
            avg_intent_count = np.mean(intent_counts)

            f.write(f"ğŸ§  æ„å›¾æ‹†è§£åˆ†æ:\n")
            f.write(f"   - å¹³å‡æ‹†è§£æ„å›¾æ•°: {avg_intent_count:.1f}\n")
            f.write(f"   - æ‹†è§£èŒƒå›´: {min(intent_counts)} - {max(intent_counts)}\n\n")

            f.write(f"ğŸ’¡ ç»“è®º:\n")
            if multi_avg_f1 > single_avg_f1:
                f.write(f"   âœ… å¤šæ„å›¾æ‹†è§£æ–¹æ³•åœ¨F1å€¼ä¸Šå¹³å‡æå‡äº† {(multi_avg_f1 - single_avg_f1) * 100:.2f}%\n")
                f.write(f"   âœ… å»ºè®®åœ¨å¤æ‚æŸ¥è¯¢åœºæ™¯ä¸­ä½¿ç”¨å¤šæ„å›¾æ‹†è§£æ–¹æ³•\n")
            else:
                f.write(f"   âš ï¸ åœ¨æ­¤æµ‹è¯•é›†ä¸Šï¼Œå¤šæ„å›¾æ‹†è§£æœªæ˜¾ç¤ºæ˜æ˜¾ä¼˜åŠ¿\n")
                f.write(f"   âš ï¸ å¯èƒ½éœ€è¦æ›´å¤§çš„æµ‹è¯•é›†æˆ–è°ƒæ•´æ‹†è§£ç­–ç•¥\n")

            f.write(
                f"   â±ï¸ å¤šæ„å›¾æ–¹æ³•å¹³å‡å¢åŠ  {((multi_avg_time - single_avg_time) / single_avg_time) * 100:.1f}% çš„æ£€ç´¢æ—¶é—´\n")

        logger.info(f"ğŸ“ æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    def run(self):
        """è¿è¡Œæ¼”ç¤º"""
        logger.info("ğŸš€ å¼€å§‹å¤šæ„å›¾æ‹†è§£æ•ˆæœæ¼”ç¤º...")
        start_time = time.time()

        try:
            results = self.demonstrate_intent_decomposition()

            total_time = time.time() - start_time
            logger.info(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
            logger.info(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’")
            logger.info(f"ğŸ“ ç»“æœç›®å½•: {self.config['results_dir']}")
            logger.info(f"ğŸ“Š å¯è§†åŒ–ç›®å½•: {self.config['vis_dir']}")

            # æ‰“å°å…³é”®å‘ç°
            if len(results) > 0:
                single_avg_f1 = np.mean([r["single_intent"]["f1"] for r in results])
                multi_avg_f1 = np.mean([r["multi_intent"]["f1"] for r in results])
                improvement = (multi_avg_f1 - single_avg_f1) * 100

                logger.info(f"\nğŸ“ˆ å…³é”®å‘ç°:")
                logger.info(f"   - å•æ„å›¾å¹³å‡F1: {single_avg_f1:.4f}")
                logger.info(f"   - å¤šæ„å›¾å¹³å‡F1: {multi_avg_f1:.4f}")
                logger.info(f"   - æ€§èƒ½æå‡: {improvement:+.2f}%")

        except Exception as e:
            logger.error(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}", exc_info=True)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å¤šæ„å›¾æ‹†è§£æ•ˆæœæ¼”ç¤º")
    print("=" * 50)

    # åˆ›å»ºæ¼”ç¤ºå™¨å¹¶è¿è¡Œ
    demo = MultiIntentDemo()
    demo.run()


if __name__ == "__main__":
    main()