#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import json
import time
import argparse
from pathlib import Path
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from dotenv import load_dotenv
from FlagEmbedding import FlagReranker
import torch
from PIL import Image
# from pdf2image import convert_from_path  # ğŸ”¥ æš‚æ—¶æ³¨é‡Šæ‰PDFè½¬æ¢
import subprocess
import pandas as pd
from collections import defaultdict
from beam_search_module import BeamSearchWrapper
import logging
import os
from dotenv import load_dotenv

# åˆ›å»ºæ—¥å¿—ç›®å½•
log_dir = Path("DeepRAG_Multimodal/log")
log_dir.mkdir(exist_ok=True)
log_file = log_dir / "multi_intent_demo.log"

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(str(log_file), mode='w', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# æ·»åŠ å¿…è¦çš„è·¯å¾„
sys.path.append("multimodal-RAG/DeepRAG_Multimodal/deep_retrieve")
# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv("D:\Desktop\multimodal-RAG\multimodal-RAG\DeepRAG_Multimodal\configs\.env")

# å¯¼å…¥å¿…è¦çš„åº“
from DeepRAG_Multimodal.deep_retrieve.ming.deepsearch_optimize_ming import DeepSearch_Beta
from DeepRAG_Multimodal.deep_retrieve.retriever_multimodal_bge import RetrieverConfig, MultimodalMatcher


class MultiIntentDemo:
    """å¤šæ„å›¾æ‹†è§£æ•ˆæœæ¼”ç¤ºç±»"""

    def __init__(self):
        """åˆå§‹åŒ–æ¼”ç¤ºå™¨"""
        self.config = self.load_config()
        os.makedirs(self.config['results_dir'], exist_ok=True)
        os.makedirs(self.config['vis_dir'], exist_ok=True)

        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False

        self.setup_models()

    def load_config(self):
        """åŠ è½½é…ç½®"""
        config = {
            # è·¯å¾„é…ç½®
            'test_data_path': r'D:\Desktop\colpali_longdoc\picked_LongDoc\selected_LongDocURL_public_with_subtask_category.jsonl',
            'pdf_base_dir': r'D:\Desktop\colpali_longdoc\picked_LongDoc',
            'results_dir': './demo_results',
            'vis_dir': './demo_results/visualizations',

            # é‡‡æ ·é…ç½®
            'sample_size': 5,
            'debug': True,

            # æ£€ç´¢é…ç½®
            'max_iterations': 2,
            'embedding_topk': 15,
            'rerank_topk': 10,
            # ğŸ”¥ ä½¿ç”¨å¹³è¡¡æƒé‡ä½†å…ˆä»¥æ–‡æœ¬ä¸ºä¸»ï¼ˆç”±äºæš‚æ—¶æ²¡æœ‰å›¾åƒï¼‰
            'text_weight': 0.8,
            'image_weight': 0.2,

            # æ¨¡å‹é…ç½®
            'mm_model_name': "vidore/colqwen2.5-v0.2",
            'mm_processor_name': "vidore/colqwen2.5-v0.1",
            'bge_model_name': "BAAI/bge-large-en-v1.5",
            'device': 'cuda:0',
            'batch_size': 2,
            # ğŸ”¥ æš‚æ—¶ä½¿ç”¨text_onlyæ¨¡å¼ï¼Œç­‰Popplerå®‰è£…åæ”¹ä¸ºmixed
            'retrieval_mode': 'text_only',
            'ocr_method': 'paddleocr',

            # ç¦ç”¨Vespa
            'use_vespa': False,

            # BeamSearché…ç½®ï¼ˆå¯é€‰ï¼‰
            'enable_beam_search': False,  # å…ˆç¦ç”¨è¿›è¡ŒåŸºç¡€æµ‹è¯•
            'beam_width': 3,
            'beam_debug': True,
        }
        return config

    def setup_models(self):
        """åˆå§‹åŒ–æ£€ç´¢æ¨¡å‹"""
        logger.info("ğŸš€ åˆå§‹åŒ–å¤šæ„å›¾æ£€ç´¢æ¨¡å‹...")
        import torch
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        logger.info(f"ğŸ® ä½¿ç”¨è®¾å¤‡: {device}")

        self.reranker = FlagReranker(
            model_name_or_path="BAAI/bge-reranker-large",
            use_fp16=True,
            device=device
        )

        # éªŒè¯rerankerè®¾å¤‡
        logger.info(f"ğŸ“ Rerankerè®¾å¤‡: {next(self.reranker.model.parameters()).device}")

        # åˆå§‹åŒ–å¤šæ¨¡æ€åŒ¹é…å™¨é…ç½®
        retriever_config = RetrieverConfig(
            model_name=self.config['mm_model_name'],
            processor_name=self.config['mm_processor_name'],
            bge_model_name=self.config['bge_model_name'],
            device=self.config['device'],
            use_fp16=True,
            batch_size=self.config['batch_size'],
            mode=self.config['retrieval_mode'],
            ocr_method=self.config['ocr_method']
        )

        self.mm_matcher = MultimodalMatcher(
            config=retriever_config,
            embedding_weight=self.config['text_weight'],
            topk=self.config['rerank_topk']
        )
        logger.info("âœ… å·²åˆå§‹åŒ–å¤šæ¨¡æ€åŒ¹é…å™¨ï¼ˆå½“å‰ä¸ºæ–‡æœ¬æ¨¡å¼ï¼‰")

        # åˆå§‹åŒ–åŸºç¡€å¤šæ„å›¾æ£€ç´¢å™¨
        base_multi_intent_search = DeepSearch_Beta(
            max_iterations=self.config['max_iterations'],
            reranker=self.reranker,
            params={
                "embedding_topk": self.config['embedding_topk'],
                "rerank_topk": self.config['rerank_topk'],
                "text_weight": self.config['text_weight'],
                "image_weight": self.config['image_weight']
            }
        )

        # BeamSearchå¯é€‰åŒ…è£…
        if self.config['enable_beam_search']:
            self.multi_intent_search = BeamSearchWrapper(
                base_retriever=base_multi_intent_search,
                matcher=self.mm_matcher,
                reranker=self.reranker,
                enable_beam_search=True,
                beam_width=self.config['beam_width'],
                debug_mode=self.config['beam_debug']
            )
            logger.info("âœ… å·²å¯ç”¨BeamSearchåŒ…è£…å™¨")
        else:
            self.multi_intent_search = base_multi_intent_search
            logger.info("âœ… ä½¿ç”¨æ ‡å‡†å¤šæ„å›¾æ£€ç´¢å™¨")

        # åˆå§‹åŒ–å•æ„å›¾æ£€ç´¢å™¨ï¼ˆç¦ç”¨æ„å›¾æ‹†è§£ï¼‰
        self.single_intent_search = DeepSearch_Beta(
            max_iterations=self.config['max_iterations'],
            reranker=self.reranker,
            params={
                "embedding_topk": self.config['embedding_topk'],
                "rerank_topk": self.config['rerank_topk'],
                "text_weight": self.config['text_weight'],
                "image_weight": self.config['image_weight']
            }
        )

        # ç¦ç”¨å•æ„å›¾æ£€ç´¢å™¨çš„æ„å›¾æ‹†è§£åŠŸèƒ½
        self.single_intent_search._split_query_intent = lambda query: [query]
        self.single_intent_search._split_query_intent_exist = lambda query: [query]

        logger.info("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    def load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        allowed_doc_nos = [
            '4046173.pdf', '4176503.pdf', '4057524.pdf', '4064501.pdf', '4057121.pdf'
        ]

        logger.info(f"ğŸ“š åŠ è½½æµ‹è¯•æ•°æ®: {self.config['test_data_path']}")
        test_data = []

        with open(self.config['test_data_path'], 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    item = json.loads(line)
                    if item.get("pdf_path") in allowed_doc_nos:
                        test_data.append(item)

        # å–æŒ‡å®šæ•°é‡çš„æµ‹è¯•æ•°æ®
        test_data = test_data[:self.config['sample_size']]
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(test_data)} æ¡æµ‹è¯•æ•°æ®")

        # æ‰“å°æµ‹è¯•æ•°æ®åŸºæœ¬ä¿¡æ¯
        for i, data in enumerate(test_data):
            logger.info(
                f"ğŸ“„ æµ‹è¯•æ•°æ® {i + 1}: {data.get('pdf_path', 'Unknown')} - {data.get('question', 'No question')[:50]}...")

        return test_data

    def process_single_document(self, doc_data):
        """ğŸ”¥ æ”¹è¿›çš„æ–‡æ¡£å¤„ç†ï¼šä¼˜å…ˆä½¿ç”¨OCRæ–‡æœ¬ï¼Œä¸ºåç»­å›¾åƒå¤„ç†åšå‡†å¤‡"""
        documents = []

        # è·å–é¢„å¤„ç†çš„OCRç»“æœ
        ocr_file = os.path.join(
            self.config['pdf_base_dir'],
            f"{self.config['ocr_method']}_save",
            f"{os.path.basename(doc_data['pdf_path']).replace('.pdf', '.json')}"
        )

        # è¯»å–é¢„å¤„ç†çš„æ–‡æœ¬æ•°æ®
        if os.path.exists(ocr_file):
            with open(ocr_file, 'r', encoding='utf-8') as f:
                loaded_data = json.load(f)
            logger.info(f"ğŸ“– æˆåŠŸè¯»å–OCRæ–‡ä»¶: {ocr_file}")
        else:
            logger.warning(f"âš ï¸ æ‰¾ä¸åˆ°OCRæ–‡ä»¶: {ocr_file}")
            # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
            logger.info(f"ğŸ”§ ä¸º {doc_data['pdf_path']} åˆ›å»ºæ¨¡æ‹ŸOCRæ•°æ®")
            loaded_data = {}
            for i in range(5):
                loaded_data[
                    f"Page_{i + 1}"] = f"è¿™æ˜¯ {doc_data['pdf_path']} ç¬¬{i + 1}é¡µçš„æ¨¡æ‹Ÿå†…å®¹ï¼ŒåŒ…å«æµ‹è¯•æ–‡æœ¬ç”¨äºæ£€ç´¢å®éªŒã€‚å…¬å¸è´¢åŠ¡æ•°æ®ï¼ŒæŠ€æœ¯ç ”å‘ä¿¡æ¯ï¼Œå¸‚åœºåˆ†æç­‰ç›¸å…³å†…å®¹ã€‚"

        # ğŸ”¥ ä¸ºæ¯ä¸€é¡µåˆ›å»ºæ–‡æ¡£å¯¹è±¡
        for idx, (page_key, page_text) in enumerate(loaded_data.items()):
            # ç¡®ä¿æ–‡æœ¬è´¨é‡
            if not page_text.strip():
                page_text = f"ç¬¬{idx + 1}é¡µå†…å®¹ - {doc_data['pdf_path']}"

            # ğŸ”¥ åˆ›å»ºæ–‡æ¡£ç»“æ„ï¼Œé¢„ç•™å›¾åƒå­—æ®µ
            documents.append({
                "text": page_text,
                "image": None,  # ğŸ”¥ æš‚æ—¶ä¸ºNoneï¼Œç­‰Popplerå®‰è£…åå¯ä»¥æ·»åŠ å›¾åƒ
                "metadata": {
                    "page_index": idx + 1,
                    "pdf_path": doc_data.get("pdf_path", "")
                }
            })

        logger.info(f"ğŸ“‘ æˆåŠŸåˆ›å»º {len(documents)} ä¸ªæ–‡æ¡£å¯¹è±¡ï¼ˆå½“å‰ä»…æ–‡æœ¬æ¨¡å¼ï¼‰")

        # æ·»åŠ æ–‡æœ¬è´¨é‡æ£€æŸ¥
        total_text_length = sum(len(doc['text']) for doc in documents)
        logger.info(f"ğŸ“ æ€»æ–‡æœ¬é•¿åº¦: {total_text_length} å­—ç¬¦")

        if total_text_length < 100:
            logger.warning(f"âš ï¸ æ–‡æ¡£æ–‡æœ¬å†…å®¹è¿‡å°‘ï¼Œå¯èƒ½å½±å“æ£€ç´¢æ•ˆæœ")
        else:
            logger.info(f"âœ… æ–‡æœ¬è´¨é‡è‰¯å¥½ï¼Œå¹³å‡æ¯é¡µ {total_text_length // len(documents)} å­—ç¬¦")

        return documents

    def demonstrate_intent_decomposition(self):
        """æ¼”ç¤ºå¤šæ„å›¾æ‹†è§£æ•ˆæœ"""
        logger.info("ğŸ¯ å¼€å§‹å¤šæ„å›¾æ‹†è§£æ•ˆæœæ¼”ç¤º...")
        test_data = self.load_test_data()
        results = []

        for idx, doc_data in enumerate(test_data):
            logger.info(f"\n{'=' * 60}")
            logger.info(f"ğŸ” å¤„ç†æ–‡æ¡£ {idx + 1}/{len(test_data)}: {doc_data.get('pdf_path', 'Unknown')}")

            query = doc_data.get("question", "")
            evidence_pages = doc_data.get("evidence_pages", [])

            logger.info(f"â“ åŸå§‹æŸ¥è¯¢: {query}")
            logger.info(f"ğŸ“‹ è¯æ®é¡µé¢: {evidence_pages}")

            # å¤„ç†æ–‡æ¡£
            document_pages = self.process_single_document(doc_data)
            if not document_pages:
                logger.warning(f"âš ï¸ è·³è¿‡æ–‡æ¡£: æ— æœ‰æ•ˆå†…å®¹")
                continue

            data = {
                "query": query,
                "documents": document_pages
            }

            # æ¼”ç¤ºæ„å›¾æ‹†è§£è¿‡ç¨‹
            intent_decomposition_result = self.analyze_intent_decomposition(query)

            # æ‰§è¡Œå•æ„å›¾æ£€ç´¢
            logger.info(f"\nğŸ” æ‰§è¡Œå•æ„å›¾æ£€ç´¢...")
            single_start = time.time()
            single_results = self.single_intent_search.search_retrieval(data, retriever=self.mm_matcher)
            single_elapsed = time.time() - single_start
            logger.info(f"â±ï¸ å•æ„å›¾æ£€ç´¢è€—æ—¶: {single_elapsed:.2f}ç§’")

            # ğŸ”¥ æ·»åŠ åˆ†æ•°è°ƒè¯•ä¿¡æ¯
            single_scores = [r.get('score', 0) for r in single_results]
            logger.info(f"ğŸ“Š å•æ„å›¾æ£€ç´¢åˆ†æ•°: {single_scores[:5]}")

            # æ‰§è¡Œå¤šæ„å›¾æ£€ç´¢
            logger.info(f"\nğŸ¯ æ‰§è¡Œå¤šæ„å›¾æ£€ç´¢...")
            multi_start = time.time()
            multi_results = self.multi_intent_search.search_retrieval(data, retriever=self.mm_matcher)
            multi_elapsed = time.time() - multi_start
            logger.info(f"â±ï¸ å¤šæ„å›¾æ£€ç´¢è€—æ—¶: {multi_elapsed:.2f}ç§’")

            # ğŸ”¥ æ·»åŠ åˆ†æ•°è°ƒè¯•ä¿¡æ¯
            multi_scores = [r.get('score', 0) for r in multi_results]
            logger.info(f"ğŸ“Š å¤šæ„å›¾æ£€ç´¢åˆ†æ•°: {multi_scores[:5]}")

            # åˆ†ææ£€ç´¢ç»“æœ
            single_analysis = self.analyze_retrieval_results(single_results, evidence_pages, "å•æ„å›¾")
            multi_analysis = self.analyze_retrieval_results(multi_results, evidence_pages, "å¤šæ„å›¾")

            # è®°å½•ç»“æœ
            result = {
                "doc_id": doc_data.get("doc_no", ""),
                "pdf_path": doc_data.get("pdf_path", ""),
                "query": query,
                "evidence_pages": evidence_pages,
                "intent_decomposition": intent_decomposition_result,
                "single_intent": {
                    **single_analysis,
                    "retrieval_time": single_elapsed,
                    "results": single_results[:5],
                    "scores": single_scores[:5]
                },
                "multi_intent": {
                    **multi_analysis,
                    "retrieval_time": multi_elapsed,
                    "results": multi_results[:5],
                    "scores": multi_scores[:5]
                }
            }

            results.append(result)

            # ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºè¯¦ç»†åˆ†æ
            self.create_detailed_analysis(result, idx + 1)

        # ä¿å­˜æ•´ä½“ç»“æœ
        result_file = os.path.join(self.config['results_dir'], 'intent_decomposition_demo.json')
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        # åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–
        self.create_comparison_visualizations(results)

        logger.info(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {self.config['results_dir']}")
        return results

    def analyze_intent_decomposition(self, query):
        """åˆ†ææ„å›¾æ‹†è§£è¿‡ç¨‹"""
        logger.info(f"\nğŸ§  åˆ†ææŸ¥è¯¢æ„å›¾æ‹†è§£è¿‡ç¨‹...")

        # è°ƒç”¨å¤šæ„å›¾æ£€ç´¢å™¨çš„æ„å›¾æ‹†è§£æ–¹æ³•
        intent_queries = self.multi_intent_search._split_query_intent(query)

        logger.info(f"ğŸ“ åŸå§‹æŸ¥è¯¢: {query}")
        logger.info(f"ğŸ¯ æ‹†è§£å‡º {len(intent_queries)} ä¸ªå­æ„å›¾:")

        for i, intent in enumerate(intent_queries, 1):
            logger.info(f"   {i}. {intent}")

        # å¦‚æœæ‹†è§£ç»“æœåªæœ‰åŸæŸ¥è¯¢ï¼Œè¯´æ˜LLMè®¤ä¸ºä¸éœ€è¦æ‹†è§£
        if len(intent_queries) == 1 and intent_queries[0] == query:
            logger.info("   ğŸ’¡ LLMåˆ¤æ–­æ­¤æŸ¥è¯¢ä¸éœ€è¦æ‹†è§£")

        # åˆ†ææ„å›¾ç±»å‹å’Œè¦†ç›–åº¦
        intent_analysis = {
            "original_query": query,
            "decomposed_intents": intent_queries,
            "intent_count": len(intent_queries),
            "coverage_analysis": self.analyze_intent_coverage(query, intent_queries)
        }

        return intent_analysis

    def analyze_intent_coverage(self, original_query, intent_queries):
        """åˆ†ææ„å›¾è¦†ç›–åº¦"""
        original_words = set(original_query.lower().split())

        coverage_stats = {
            "original_word_count": len(original_words),
            "intent_coverage": []
        }

        for intent in intent_queries:
            intent_words = set(intent.lower().split())
            overlap = original_words.intersection(intent_words)
            coverage = len(overlap) / len(original_words) if original_words else 0

            coverage_stats["intent_coverage"].append({
                "intent": intent,
                "word_overlap": len(overlap),
                "coverage_ratio": coverage,
                "new_words": list(intent_words - original_words)
            })

        return coverage_stats

    def analyze_retrieval_results(self, results, evidence_pages, method_name):
        """åˆ†ææ£€ç´¢ç»“æœ"""
        retrieved_pages = set()
        for result in results:
            if 'page' in result and result['page'] is not None:
                retrieved_pages.add(result['page'])
            elif 'metadata' in result and 'page_index' in result['metadata']:
                retrieved_pages.add(result['metadata']['page_index'])

        evidence_set = set(evidence_pages)
        correct_pages = evidence_set.intersection(retrieved_pages)

        recall = len(correct_pages) / len(evidence_set) if evidence_set else 0
        precision = len(correct_pages) / len(retrieved_pages) if retrieved_pages else 0
        f1 = 2 * recall * precision / (recall + precision) if (recall + precision) > 0 else 0

        logger.info(f"ğŸ“Š {method_name}æ£€ç´¢ç»“æœåˆ†æ:")
        logger.info(f"   ğŸ¯ æ£€ç´¢åˆ°é¡µé¢: {sorted(list(retrieved_pages))}")
        logger.info(f"   âœ… æ­£ç¡®é¡µé¢: {sorted(list(correct_pages))}")
        logger.info(f"   ğŸ“ˆ å¬å›ç‡: {recall:.4f}")
        logger.info(f"   ğŸ“ˆ ç²¾ç¡®ç‡: {precision:.4f}")
        logger.info(f"   ğŸ“ˆ F1å€¼: {f1:.4f}")

        return {
            "retrieved_pages": list(retrieved_pages),
            "correct_pages": list(correct_pages),
            "recall": recall,
            "precision": precision,
            "f1": f1,
            "success": len(correct_pages) == len(evidence_set)
        }

    def create_detailed_analysis(self, result, doc_index):
        """ä¸ºå•ä¸ªæ–‡æ¡£åˆ›å»ºè¯¦ç»†åˆ†æå›¾è¡¨"""
        # æš‚æ—¶ç®€åŒ–ï¼Œé¿å…matplotlibå¯èƒ½çš„ä¾èµ–é—®é¢˜
        logger.info(f"ğŸ“Š æ–‡æ¡£ {doc_index} åˆ†æå·²è®°å½•")

    def create_comparison_visualizations(self, results):
        """åˆ›å»ºæ•´ä½“å¯¹æ¯”å¯è§†åŒ–"""
        # æš‚æ—¶ç®€åŒ–ï¼Œé¿å…matplotlibå¯èƒ½çš„ä¾èµ–é—®é¢˜
        logger.info(f"ğŸ“Š å¯¹æ¯”å¯è§†åŒ–å·²è®°å½•")

    def run(self):
        """è¿è¡Œæ¼”ç¤º"""
        logger.info("ğŸš€ å¼€å§‹å¤šæ„å›¾æ‹†è§£æ•ˆæœæ¼”ç¤º...")
        logger.info("âš ï¸ å½“å‰è¿è¡Œåœ¨æ–‡æœ¬æ¨¡å¼ä¸‹ï¼Œå®‰è£…Poppleråå¯å¯ç”¨å®Œæ•´å¤šæ¨¡æ€åŠŸèƒ½")
        start_time = time.time()

        try:
            results = self.demonstrate_intent_decomposition()

            total_time = time.time() - start_time
            logger.info(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
            logger.info(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’")
            logger.info(f"ğŸ“ ç»“æœç›®å½•: {self.config['results_dir']}")
            logger.info(f"ğŸ“Š å¯è§†åŒ–ç›®å½•: {self.config['vis_dir']}")

            # æ‰“å°å…³é”®å‘ç°
            if len(results) > 0:
                single_avg_f1 = np.mean([r["single_intent"]["f1"] for r in results])
                multi_avg_f1 = np.mean([r["multi_intent"]["f1"] for r in results])
                improvement = (multi_avg_f1 - single_avg_f1) * 100

                logger.info(f"\nğŸ“ˆ å…³é”®å‘ç°:")
                logger.info(f"   - å•æ„å›¾å¹³å‡F1: {single_avg_f1:.4f}")
                logger.info(f"   - å¤šæ„å›¾å¹³å‡F1: {multi_avg_f1:.4f}")
                logger.info(f"   - æ€§èƒ½æå‡: {improvement:+.2f}%")

                # åˆ†æ•°è´¨é‡æ£€æŸ¥
                single_max_score = max(
                    [max(r["single_intent"]["scores"]) for r in results if r["single_intent"]["scores"]])
                multi_max_score = max(
                    [max(r["multi_intent"]["scores"]) for r in results if r["multi_intent"]["scores"]])

                logger.info(f"\nğŸ“Š åˆ†æ•°è´¨é‡æ£€æŸ¥:")
                logger.info(f"   - å•æ„å›¾æœ€é«˜åˆ†æ•°: {single_max_score:.4f}")
                logger.info(f"   - å¤šæ„å›¾æœ€é«˜åˆ†æ•°: {multi_max_score:.4f}")

                if single_max_score > 0 and multi_max_score > 0:
                    logger.info(f"   âœ… æ£€ç´¢åŠŸèƒ½æ­£å¸¸ï¼Œåˆ†æ•°ä¸ä¸º0")
                else:
                    logger.warning(f"   âš ï¸ æ£€ç´¢åˆ†æ•°å¼‚å¸¸ï¼Œéœ€è¦æ£€æŸ¥é…ç½®")

        except Exception as e:
            logger.error(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}", exc_info=True)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å¤šæ„å›¾æ‹†è§£æ•ˆæœæ¼”ç¤ºï¼ˆæ–‡æœ¬æ¨¡å¼ï¼‰")
    print("=" * 50)
    print("ğŸ’¡ æç¤ºï¼šå®‰è£…Poppleråå¯å¯ç”¨å®Œæ•´å¤šæ¨¡æ€åŠŸèƒ½")
    print("   conda install -c conda-forge poppler")
    print("=" * 50)

    # åˆ›å»ºæ¼”ç¤ºå™¨å¹¶è¿è¡Œ
    demo = MultiIntentDemo()
    demo.run()


if __name__ == "__main__":
    main()